{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview The HPE SimpliVity Container Storage Interface (CSI) driver for vSphere enables you to provision HPE SimpliVity storage for Kubernetes applications while providing HPE SimpliVity data protection. It includes the following features: Snapshots Static and dynamic provisioning Read and write from a single pod Components The following diagram illustrates the components in a Kubernetes cluster with the HPE SimpliVity CSI driver deployed. Kubernetes cluster : The set of Kubernetes nodes used to run containerized applications on an HPE OmniStack host. HPE SimpliVity Container Storage Interface (CSI) driver : The HPE SimpliVity software component that is responsible for provisioning persistent volumes from an existing HPE SimpliVity datastore. vCenter Server CNS driver : A vCenter Server component that enables provisioning and life cycle operations for container volumes on vSphere. The HPE SimpliVity CSI plugin uses the CNS driver to create and delete persistent volumes. First Class Disk (FCD) : An FCD is a volume that is independent of a virtual machine in VMware. The HPE SimpliVity CSI Driver uses FCDs to manage data volumes separate from Kubernetes nodes. HPE OmniStack host : An x86 server running HPE OmniStack software and VMware ESXi. To use the HPE SimpliVity CSI plugin, the HPE OmniStack host must be running VMware ESXi 6.7U3 or higher. HPE SimpliVity datastore : HPE SimpliVity datastores provide storage resources to the VMware ESXi host over the NFS protocol. A Kubernetes storage class maps persistent volume requests to an HPE SimpliVity datastore. Persistent volumes inherit the backup policy of the datastore where they are created. REST API : The HPE SimpliVity CSI plugin uses an internal REST API to create and restore persistent volume snapshots in an HPE SimpliVity datastore. CLI : The HPE SimpliVity CLI has been enhanced to support persistent volumes. You can use the CLI to create an HPE SimpliVity backup of a persistent volume (svt-pv-backup), to list the set of existing PVs (svt-pv-show) or to change the default backup policy for a PV (svt-pv-policy-set). You can also backup and restore a PV (svt-backup-show and svt-backup-restore). HPE OmniStack requires that the PV name be in the format of pvc-<uid>_fcd .","title":"Overview"},{"location":"#overview","text":"The HPE SimpliVity Container Storage Interface (CSI) driver for vSphere enables you to provision HPE SimpliVity storage for Kubernetes applications while providing HPE SimpliVity data protection. It includes the following features: Snapshots Static and dynamic provisioning Read and write from a single pod","title":"Overview"},{"location":"#components","text":"The following diagram illustrates the components in a Kubernetes cluster with the HPE SimpliVity CSI driver deployed. Kubernetes cluster : The set of Kubernetes nodes used to run containerized applications on an HPE OmniStack host. HPE SimpliVity Container Storage Interface (CSI) driver : The HPE SimpliVity software component that is responsible for provisioning persistent volumes from an existing HPE SimpliVity datastore. vCenter Server CNS driver : A vCenter Server component that enables provisioning and life cycle operations for container volumes on vSphere. The HPE SimpliVity CSI plugin uses the CNS driver to create and delete persistent volumes. First Class Disk (FCD) : An FCD is a volume that is independent of a virtual machine in VMware. The HPE SimpliVity CSI Driver uses FCDs to manage data volumes separate from Kubernetes nodes. HPE OmniStack host : An x86 server running HPE OmniStack software and VMware ESXi. To use the HPE SimpliVity CSI plugin, the HPE OmniStack host must be running VMware ESXi 6.7U3 or higher. HPE SimpliVity datastore : HPE SimpliVity datastores provide storage resources to the VMware ESXi host over the NFS protocol. A Kubernetes storage class maps persistent volume requests to an HPE SimpliVity datastore. Persistent volumes inherit the backup policy of the datastore where they are created. REST API : The HPE SimpliVity CSI plugin uses an internal REST API to create and restore persistent volume snapshots in an HPE SimpliVity datastore. CLI : The HPE SimpliVity CLI has been enhanced to support persistent volumes. You can use the CLI to create an HPE SimpliVity backup of a persistent volume (svt-pv-backup), to list the set of existing PVs (svt-pv-show) or to change the default backup policy for a PV (svt-pv-policy-set). You can also backup and restore a PV (svt-backup-show and svt-backup-restore). HPE OmniStack requires that the PV name be in the format of pvc-<uid>_fcd .","title":"Components"},{"location":"block-volumes/","text":"HPE SimpliVity CSI Driver - Block Volumes There are two types of volume provisioning in a Kubernetes cluster: Dynamically Provisioning a Volume Statically Provisioning a Volume Dynamically Provisioning a Volume Dynamically provisioning a volume allows storage volumes to be created on-demand. The dynamic provisioning eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by a user. The implementation of dynamic volume provisioning is based on the API object StorageClass from the API group storage.k8s.io. A cluster administrator can define as many StorageClass objects as needed, each specifying a volume plugin (a.k.a provisioner) that provisions a volume and a set of parameters to that provisioner when provisioning. A cluster administrator can define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. How to Dynamically Provision a Block Volume on a Kubernetes Cluster Define a StorageClass for the HPE SimpliVity CSI driver as the provisioner. A datastore URL or storage policy can be used to further restrict which datastore will be used by this storage class. In this case, neither are specified which means the driver will select the SimpliVity datastore with the most free space: # Contents of example-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. # datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter # storagepolicyname: \"policy-name\" # Policy on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" Create this StorageClass into the Kubernetes Cluster: $ kubectl create -f example-sc.yaml Define a PersistentVolumeClaim: # Contents of example-dynamic-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: example-dynamic-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: simplivity-sc Create this PersistentVolumeClaim in the Kubernetes Cluster: $ kubectl create -f example-dynamic-pvc.yaml A PersistentVolume is dynamically created and is bound to this PersistentVolumeClaim. Verify that the PersistentVolumeClaim was created and a PersistentVolume is attached to it. The base status should show Bound if it worked and the Volume field should be populated. $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-dynamic-pvc Bound pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da 5Gi RWO simplivity-sc 2m19s Details of the PVC $ kubectl describe pvc example-dynamic-pvc Name: example-dynamic-pvc Namespace: default StorageClass: simplivity-sc Status: Bound Volume: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 5Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 3m48s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 External provisioner is provisioning volume for claim \"default/example-dynamic-pvc\" Normal ExternalProvisioning 3m8s (x4 over 3m48s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.simplivity.hpe.com\" or manually created by system administrator Normal ProvisioningSucceeded 2m56s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 Successfully provisioned volume pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Here, ReadWriteOnce (RWO) access mode indicates that the volume provisioned is a Block Volume. In both the basic get information and the detailed describe information of the PVC, the volume is shown in the Volume field. $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da 5Gi RWO Delete Bound default/example-dynamic-pvc simplivity-sc 8m27s $ kubectl describe pv pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Name: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: simplivity-sc Status: Bound Claim: default/example-dynamic-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 5Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.simplivity.hpe.com VolumeHandle: d3cff17d-334a-4217-b55d-e478648c5b8d ReadOnly: false VolumeAttributes: datastoreurl=ds:///vmfs/volumes/188e6f22-173b61ae/ fstype=ext4 storage.kubernetes.io/csiProvisionerIdentity=1589547226925-8081-csi.simplivity.hpe.com type=HPE SimpliVity CNS Block Volume Events: <none> The Status: in both the outputs say bound and the Claim: points to the PVC that was created earlier default/example-dynamic-pvc . This created a new volume and attached it to the application for the user. Statically Provisioning a Volume Use this method when there is already an existing volume that is wanted by the Kubernetes cluster. Static provisioning is a feature that is native to Kubernetes and that allows cluster administrators to make existing storage devices available to a cluster. As a cluster administrator, you must know the details of the storage device, its supported configurations, and mount options. To make existing storage available to a cluster user, you must manually create the storage device, a PeristentVolume, and a PersistentVolumeClaim. Because the PV and the storage device already exists, there is no need to specify a storage class name in the PVC spec. There are many ways to create a static PV and PVC binding, some examples are label matching, volume size matching etc... Use Cases of Static Provisioning Common use cases supported for static volume provisioning: Use an existing storage device: You provisioned a persistent storage First Class Disk (FCD) directly in your VC and want to use this FCD in your cluster. Make retained data available to the cluster: You provisioned a volume with a reclaimPolicy: retain in the storage class by using dynamic provisioning. You removed the PVC, but the PV, the physical storage in the VC, and the data still exist. You want to access the retained data from the same or another app in your cluster. How to Statically Provision a Block Volume on a Kubernetes Cluster First a FCD ID will need to be obtained for the volume. If the volume is not already a FCD it will need to be registered as one. This command will regiseter the volume as a FCD and return the FCD ID $ govc disk.register -ds=<datastore name> <myVMDKdirectory/mydisk.vmdk> <new FCD name> If the volume is already a FCD, the FCD ID can be retrieved with the PowerCLI or govc utility PowerCLI $ Connect-VIServer <vCenter ip> -User administrator@vsphere.local -Password <vCenter password> $ (Get-VDisk -Name <new FCD name>).id.Split(':')[1] 3515afd9-150e-4603-9267-7f0ff165ae63 GOVC $ govc disk.ls 3515afd9-150e-4603-9267-7f0ff165ae63 static-pv Define a PersistentVolume (PV) that links a FCD to Kubernetes apiVersion: v1 kind: PersistentVolume metadata: name: static-pv-name # Set name of new PV labels: \"fcd-id\": \"3515afd9-150e-4603-9267-7f0ff165ae63\" # This label is used as selector to bind with volume claim. # This can we any unique key-value to identify PV. spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain csi: driver: csi.simplivity.hpe.com # make sure this says `csi.simplivity.hpe.com` fsType: ext4 volumeAttributes: fstype: \"\" storage.kubernetes.io/csiProvisionerIdentity: static-pv-name-csi.simplivity.hpe.com # update this type: \"vSphere CNS Block Volume\" datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter volumeHandle: 3515afd9-150e-4603-9267-7f0ff165ae63 # First Class Disk (Improved Virtual Disk) ID Define a PersistentVolumeClaim (PVC), which makes the PV useable by another Kubernetes resource like a Pod or StatefulSet. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: static-pvc-name spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi selector: matchLabels: fcd-id: 3515afd9-150e-4603-9267-7f0ff165ae63 # This label is used as selector to find matching PV with specified key and value. storageClassName: \"\" Create a static PV $ kubectl apply -f static-pv.yaml persistentvolume/static-pv-name created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE static-pv-name 1Gi RWO Retain Available 5s Create PVC for the static PV. The PVC will be bound to the PV in the Volume column. While the PV will be updated with a Claim and the Status will be updated to Bound . $ kubectl apply -f static-pvc.yaml persistentvolumeclaim/static-pvc-name created $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE static-pvc-name Bound static-pv-name 1Gi RWO 8s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE static-pv-name 1Gi RWO Retain Bound default/static-pvc-name 2m The PV is now ready to be used by Kubernetes through the PVC it is bound to.","title":"Block Volumes"},{"location":"block-volumes/#hpe-simplivity-csi-driver-block-volumes","text":"There are two types of volume provisioning in a Kubernetes cluster: Dynamically Provisioning a Volume Statically Provisioning a Volume","title":"HPE SimpliVity CSI Driver - Block Volumes"},{"location":"block-volumes/#dynamically-provisioning-a-volume","text":"Dynamically provisioning a volume allows storage volumes to be created on-demand. The dynamic provisioning eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by a user. The implementation of dynamic volume provisioning is based on the API object StorageClass from the API group storage.k8s.io. A cluster administrator can define as many StorageClass objects as needed, each specifying a volume plugin (a.k.a provisioner) that provisions a volume and a set of parameters to that provisioner when provisioning. A cluster administrator can define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters.","title":"Dynamically Provisioning a Volume "},{"location":"block-volumes/#how-to-dynamically-provision-a-block-volume-on-a-kubernetes-cluster","text":"Define a StorageClass for the HPE SimpliVity CSI driver as the provisioner. A datastore URL or storage policy can be used to further restrict which datastore will be used by this storage class. In this case, neither are specified which means the driver will select the SimpliVity datastore with the most free space: # Contents of example-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. # datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter # storagepolicyname: \"policy-name\" # Policy on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" Create this StorageClass into the Kubernetes Cluster: $ kubectl create -f example-sc.yaml Define a PersistentVolumeClaim: # Contents of example-dynamic-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: example-dynamic-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: simplivity-sc Create this PersistentVolumeClaim in the Kubernetes Cluster: $ kubectl create -f example-dynamic-pvc.yaml A PersistentVolume is dynamically created and is bound to this PersistentVolumeClaim. Verify that the PersistentVolumeClaim was created and a PersistentVolume is attached to it. The base status should show Bound if it worked and the Volume field should be populated. $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-dynamic-pvc Bound pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da 5Gi RWO simplivity-sc 2m19s Details of the PVC $ kubectl describe pvc example-dynamic-pvc Name: example-dynamic-pvc Namespace: default StorageClass: simplivity-sc Status: Bound Volume: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 5Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 3m48s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 External provisioner is provisioning volume for claim \"default/example-dynamic-pvc\" Normal ExternalProvisioning 3m8s (x4 over 3m48s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.simplivity.hpe.com\" or manually created by system administrator Normal ProvisioningSucceeded 2m56s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 Successfully provisioned volume pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Here, ReadWriteOnce (RWO) access mode indicates that the volume provisioned is a Block Volume. In both the basic get information and the detailed describe information of the PVC, the volume is shown in the Volume field. $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da 5Gi RWO Delete Bound default/example-dynamic-pvc simplivity-sc 8m27s $ kubectl describe pv pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Name: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: simplivity-sc Status: Bound Claim: default/example-dynamic-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 5Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.simplivity.hpe.com VolumeHandle: d3cff17d-334a-4217-b55d-e478648c5b8d ReadOnly: false VolumeAttributes: datastoreurl=ds:///vmfs/volumes/188e6f22-173b61ae/ fstype=ext4 storage.kubernetes.io/csiProvisionerIdentity=1589547226925-8081-csi.simplivity.hpe.com type=HPE SimpliVity CNS Block Volume Events: <none> The Status: in both the outputs say bound and the Claim: points to the PVC that was created earlier default/example-dynamic-pvc . This created a new volume and attached it to the application for the user.","title":"How to Dynamically Provision a Block Volume on a Kubernetes Cluster"},{"location":"block-volumes/#statically-provisioning-a-volume","text":"Use this method when there is already an existing volume that is wanted by the Kubernetes cluster. Static provisioning is a feature that is native to Kubernetes and that allows cluster administrators to make existing storage devices available to a cluster. As a cluster administrator, you must know the details of the storage device, its supported configurations, and mount options. To make existing storage available to a cluster user, you must manually create the storage device, a PeristentVolume, and a PersistentVolumeClaim. Because the PV and the storage device already exists, there is no need to specify a storage class name in the PVC spec. There are many ways to create a static PV and PVC binding, some examples are label matching, volume size matching etc...","title":"Statically Provisioning a Volume "},{"location":"block-volumes/#use-cases-of-static-provisioning","text":"Common use cases supported for static volume provisioning: Use an existing storage device: You provisioned a persistent storage First Class Disk (FCD) directly in your VC and want to use this FCD in your cluster. Make retained data available to the cluster: You provisioned a volume with a reclaimPolicy: retain in the storage class by using dynamic provisioning. You removed the PVC, but the PV, the physical storage in the VC, and the data still exist. You want to access the retained data from the same or another app in your cluster.","title":"Use Cases of Static Provisioning"},{"location":"block-volumes/#how-to-statically-provision-a-block-volume-on-a-kubernetes-cluster","text":"First a FCD ID will need to be obtained for the volume. If the volume is not already a FCD it will need to be registered as one. This command will regiseter the volume as a FCD and return the FCD ID $ govc disk.register -ds=<datastore name> <myVMDKdirectory/mydisk.vmdk> <new FCD name> If the volume is already a FCD, the FCD ID can be retrieved with the PowerCLI or govc utility PowerCLI $ Connect-VIServer <vCenter ip> -User administrator@vsphere.local -Password <vCenter password> $ (Get-VDisk -Name <new FCD name>).id.Split(':')[1] 3515afd9-150e-4603-9267-7f0ff165ae63 GOVC $ govc disk.ls 3515afd9-150e-4603-9267-7f0ff165ae63 static-pv Define a PersistentVolume (PV) that links a FCD to Kubernetes apiVersion: v1 kind: PersistentVolume metadata: name: static-pv-name # Set name of new PV labels: \"fcd-id\": \"3515afd9-150e-4603-9267-7f0ff165ae63\" # This label is used as selector to bind with volume claim. # This can we any unique key-value to identify PV. spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain csi: driver: csi.simplivity.hpe.com # make sure this says `csi.simplivity.hpe.com` fsType: ext4 volumeAttributes: fstype: \"\" storage.kubernetes.io/csiProvisionerIdentity: static-pv-name-csi.simplivity.hpe.com # update this type: \"vSphere CNS Block Volume\" datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter volumeHandle: 3515afd9-150e-4603-9267-7f0ff165ae63 # First Class Disk (Improved Virtual Disk) ID Define a PersistentVolumeClaim (PVC), which makes the PV useable by another Kubernetes resource like a Pod or StatefulSet. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: static-pvc-name spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi selector: matchLabels: fcd-id: 3515afd9-150e-4603-9267-7f0ff165ae63 # This label is used as selector to find matching PV with specified key and value. storageClassName: \"\" Create a static PV $ kubectl apply -f static-pv.yaml persistentvolume/static-pv-name created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE static-pv-name 1Gi RWO Retain Available 5s Create PVC for the static PV. The PVC will be bound to the PV in the Volume column. While the PV will be updated with a Claim and the Status will be updated to Bound . $ kubectl apply -f static-pvc.yaml persistentvolumeclaim/static-pvc-name created $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE static-pvc-name Bound static-pv-name 1Gi RWO 8s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE static-pv-name 1Gi RWO Retain Bound default/static-pvc-name 2m The PV is now ready to be used by Kubernetes through the PVC it is bound to.","title":"How to Statically Provision a Block Volume on a Kubernetes Cluster"},{"location":"known-issues/","text":"Known Issues Storage Migration Persistent Volumes (PVs) should not be migrated while attached to a worker node. Potential issues range from needing to copy the data to a new PV to losing the data permanently. To avoid this some preparation needs to be completed before migrating to a different datastore or different ESXi cluster, proceed in this order: Drain the node from Kubernetes. Migrate the VM. Uncordon the node from Kubernetes. If the possiblity of migration is a concern there is a way to disable all migration on each worker node. This script can be used for disabling methods. Disable the \"RelocateVM Task\" method for the specific VM, which disables both compute and storage migration.","title":"Known Issues"},{"location":"known-issues/#known-issues","text":"","title":"Known Issues"},{"location":"known-issues/#storage-migration","text":"Persistent Volumes (PVs) should not be migrated while attached to a worker node. Potential issues range from needing to copy the data to a new PV to losing the data permanently. To avoid this some preparation needs to be completed before migrating to a different datastore or different ESXi cluster, proceed in this order: Drain the node from Kubernetes. Migrate the VM. Uncordon the node from Kubernetes. If the possiblity of migration is a concern there is a way to disable all migration on each worker node. This script can be used for disabling methods. Disable the \"RelocateVM Task\" method for the specific VM, which disables both compute and storage migration.","title":"Storage Migration"},{"location":"quickstart/","text":"Quickstart Guide for HPE SimpliVity CSI Driver Prerequisites must be installed prior to following this quickstart guide. Create a Storage Class Create a storage class that points to the desired datastore using either the datastore URL or the storage policy name. Below is an example of a storageclass.yaml # Contents of storageclass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter storagepolicyname: \"policy-name\" # Policy on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" $ kubectl apply -f storageclass.yaml $ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE simplivity-sc csi.simplivity.hpe.com Delete Immediate false 9m2s Dynamically Create a Volume In a StatefulSet or Pod definition define a volume claim that points to the storage class. Below is an example that uses Nginx with a Persistent Volume (PV) that uses the previously defined storage class. # Content of nginx-ss.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx spec: serviceName: nginx-service replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginxd-container image: nginx resources: requests: cpu: 0.2 memory: 200Mi ports: - containerPort: 80 volumeMounts: - name: nginx-pvc mountPath: /data/ng volumeClaimTemplates: - metadata: name: nginx-pvc annotations: volume.beta.kubernetes.io/storage-class: \"simplivity-sc\" # Name of StorageClass spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Apply the StatefulSet to create the resources. This will take a little while as the volume will need to be created as part of the deployment in this example $ kubectl apply -f nginx-ss.yaml statefulset.apps/nginx created $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-0 1/1 Running 0 9m57s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nginx-pvc-nginx-0 Bound pvc-ff1dbada-3cfc-40c1-b5ee-ee9669e5a3ed 1Gi RWO simplivity-sc 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-ff1dbada-3cfc-40c1-b5ee-ee9669e5a3ed 1Gi RWO Delete Bound default/nginx-pvc-nginx-0 simplivity-sc 10m Lets add some data to the Volume $ kubectl exec -it nginx-0 -- sh # echo \"Nginx is Alive!!\" > /data/ng/index.html # exit Now we have added data to a persistent volume in the Nginx app.","title":"Quickstart Guide"},{"location":"quickstart/#quickstart-guide-for-hpe-simplivity-csi-driver","text":"Prerequisites must be installed prior to following this quickstart guide.","title":"Quickstart Guide for HPE SimpliVity CSI Driver"},{"location":"quickstart/#create-a-storage-class","text":"Create a storage class that points to the desired datastore using either the datastore URL or the storage policy name. Below is an example of a storageclass.yaml # Contents of storageclass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter storagepolicyname: \"policy-name\" # Policy on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" $ kubectl apply -f storageclass.yaml $ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE simplivity-sc csi.simplivity.hpe.com Delete Immediate false 9m2s","title":"Create a Storage Class"},{"location":"quickstart/#dynamically-create-a-volume","text":"In a StatefulSet or Pod definition define a volume claim that points to the storage class. Below is an example that uses Nginx with a Persistent Volume (PV) that uses the previously defined storage class. # Content of nginx-ss.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx spec: serviceName: nginx-service replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginxd-container image: nginx resources: requests: cpu: 0.2 memory: 200Mi ports: - containerPort: 80 volumeMounts: - name: nginx-pvc mountPath: /data/ng volumeClaimTemplates: - metadata: name: nginx-pvc annotations: volume.beta.kubernetes.io/storage-class: \"simplivity-sc\" # Name of StorageClass spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Apply the StatefulSet to create the resources. This will take a little while as the volume will need to be created as part of the deployment in this example $ kubectl apply -f nginx-ss.yaml statefulset.apps/nginx created $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-0 1/1 Running 0 9m57s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nginx-pvc-nginx-0 Bound pvc-ff1dbada-3cfc-40c1-b5ee-ee9669e5a3ed 1Gi RWO simplivity-sc 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-ff1dbada-3cfc-40c1-b5ee-ee9669e5a3ed 1Gi RWO Delete Bound default/nginx-pvc-nginx-0 simplivity-sc 10m Lets add some data to the Volume $ kubectl exec -it nginx-0 -- sh # echo \"Nginx is Alive!!\" > /data/ng/index.html # exit Now we have added data to a persistent volume in the Nginx app.","title":"Dynamically Create a Volume"},{"location":"setup/","text":"Configure the HPE SimpliVity datastore to support persistent volumes A storage policy can be used to restrict persitent volume provisioning to a subset of HPE SimpliVity datastores. Since HPE SimpliVity datastores are NFS datastores, the vSphere storage policy must use tag based placement. You use the vSphere client to define the tag, assign the tag to the HPE SimpliVity datastore, and to create a storage policy that uses the tag. Once that is done, you update those values in the Kubernetes StorageClass.yaml . Tag the datastore Use the vSphere client to access the Tags & Custom Attributes wizard. Create a new tag and supply a meaningful name and description that will help you identify the datastore. Next, browse to the HPE SimpliVity datastore, and assign the tag to it. Right click the datastore and select Tags & Custom Attributes > Assign Tag . Create the storage policy Use the Create VM Storage Policy wizard to create the storage policy. (Go to Menu > Policies and Profiles ). Make sure to select Enable tag based placement rules . Use the same tag that you assigned to the HPE SimpliVity datastore. Define a Kubernetes StorageClass After you create the storage policies, create a StorageClass.yaml that maps to this storage type. You can specify either the datastoreurl or the storagepolicyname . If you specify both, the datastoreurl takes precedence. To get the value for the datastoreurl parameter: Navigate to the datastore. Click the Summary tab. Copy the URL value and add it to the StorageClass.yml file as shown below: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. If both are # provided, then the datastoreurl is preferred over storagepolicyname datastoreurl: \"ds:///vmfs/volumes/208f9178-aadd745f/\" # Optional Parameter fstype: \"ext4\" This an example of the yaml with the storagepolicyname: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. If both are # provided, then the datastoreurl is preferred over storagepolicyname storagepolicyname: \"DailyBackups\" # Tag on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" Monitor persistent volumes in vSphere client Use the vSphere client to see how Kubernetes has provisioned the HPE SimpliVity datastore that you mapped to your Kubernetes cluster. Navigate to the vCenter Server instance, a data center, or datastore. Click the Monitor tab, then click Container Volumes under Cloud Native Storage to view the details about the persistent volumes that are consuming storage. Click the Details icon to access the details about the Kubernetes objects associated with the persistent volume.","title":"Setup HPE SimpliVity for CSI"},{"location":"setup/#configure-the-hpe-simplivity-datastore-to-support-persistent-volumes","text":"A storage policy can be used to restrict persitent volume provisioning to a subset of HPE SimpliVity datastores. Since HPE SimpliVity datastores are NFS datastores, the vSphere storage policy must use tag based placement. You use the vSphere client to define the tag, assign the tag to the HPE SimpliVity datastore, and to create a storage policy that uses the tag. Once that is done, you update those values in the Kubernetes StorageClass.yaml .","title":"Configure the HPE SimpliVity datastore to support persistent volumes"},{"location":"setup/#tag-the-datastore","text":"Use the vSphere client to access the Tags & Custom Attributes wizard. Create a new tag and supply a meaningful name and description that will help you identify the datastore. Next, browse to the HPE SimpliVity datastore, and assign the tag to it. Right click the datastore and select Tags & Custom Attributes > Assign Tag .","title":"Tag the datastore"},{"location":"setup/#create-the-storage-policy","text":"Use the Create VM Storage Policy wizard to create the storage policy. (Go to Menu > Policies and Profiles ). Make sure to select Enable tag based placement rules . Use the same tag that you assigned to the HPE SimpliVity datastore.","title":"Create the storage policy"},{"location":"setup/#define-a-kubernetes-storageclass","text":"After you create the storage policies, create a StorageClass.yaml that maps to this storage type. You can specify either the datastoreurl or the storagepolicyname . If you specify both, the datastoreurl takes precedence. To get the value for the datastoreurl parameter: Navigate to the datastore. Click the Summary tab. Copy the URL value and add it to the StorageClass.yml file as shown below: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. If both are # provided, then the datastoreurl is preferred over storagepolicyname datastoreurl: \"ds:///vmfs/volumes/208f9178-aadd745f/\" # Optional Parameter fstype: \"ext4\" This an example of the yaml with the storagepolicyname: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com parameters: # datastoreurl and storagepolicyname are mutually exclusive. If both are # provided, then the datastoreurl is preferred over storagepolicyname storagepolicyname: \"DailyBackups\" # Tag on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\"","title":"Define a Kubernetes StorageClass"},{"location":"setup/#monitor-persistent-volumes-in-vsphere-client","text":"Use the vSphere client to see how Kubernetes has provisioned the HPE SimpliVity datastore that you mapped to your Kubernetes cluster. Navigate to the vCenter Server instance, a data center, or datastore. Click the Monitor tab, then click Container Volumes under Cloud Native Storage to view the details about the persistent volumes that are consuming storage. Click the Details icon to access the details about the Kubernetes objects associated with the persistent volume.","title":"Monitor persistent volumes in vSphere client"},{"location":"snapshot/","text":"HPE SimpliVity CSI Driver - Volume Snapshots For dynamic volume creation, creating a PersistentVolumeClaim (PVC) initiates the creation of a PersistentVolume (PV) which contains the data. A PVC also specifies a StorageClass which provides additional attributes (e.g. SimpliVity datastore). The CSI snapshot feature follows the same pattern. Creating a VolumeSnapshot triggers creation of a VolumeSnapshotContent object which contains the snapshot data. A VolumeSnapshot also specifies a VolumeSnapshotClass to provide additional attributes (e.g. retention policy). VolumeSnapshotContent : The Kubernetes cluster resource that represents a snapshot of a persistent volume. VolumeSnapshot : A snapshot request. Creating a VolumeSnapshot triggers a snapshot (VolumeSnapshotContent) and the objects are bound together. There is a one-to-one mapping between VolumeSnapshot and VolumeSnapshotContent objects. VolumeSnapshotClass : Dynamically provisioned snapshots specify a VolumeSnapshotClass to provide additional parameters for the snapshot operation. This is similar to how a StorageClass is used for PVs, and allows volumes created from the same StorageClass to be snapshotted with different options (e.g. retention policy). Volume Snapshot Provisioning There are two types of snapshot provisioning in a Kubernetes cluster: Dynamic Volume Snapshot : Create an on-demand snapshot. Static Volume Snapshot : Create a snapshot for a pre-existing HPE SimpliVity snapshot. For example, to create a Kubernetes snapshot from a scheduled backup. Dynamic Volume Snapshot Volume snapshots of HPE SimpliVity volumes can be created on demand using the Kubernetes CLI. To do so, create a VolumeSnapshot object containing the PersistentVolumeClaim that you wish to snap. Kubernetes does not quiesce IO during a snapshot operation and thus the snapshot will just be crash consistent. To take an application consistent snapshot, you must take additional steps (e.g. shutdown the pod consuming the PV). Dynamic Snapshot Procedure Define a VolumeSnapshotClass. The deletionPolicy enables you to configure what happens to a VolumeSnapshotContent object when the VolumeSnapshot object to which it is bound is deleted. The deletionPolicy of a volume snapshot can either be Retain or Delete . apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: hpe-simplivity-snapclass driver: csi.simplivity.hpe.com deletionPolicy: Delete parameters: description: \"Snapshot created by the HPE SimpliVity vSphere CSI Driver\" Create the VolumeSnapshotClass object. $ kubectl create -f example-snapshotclass.yaml Define a VolumeSnapshot object. volumeSnapshotClassName specifies the snapshot options (defined in step 2) and persistentVolumeClaim identifies the snapshot source volume. apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: www-web-snap-1 spec: volumeSnapshotClassName: hpe-simplivity-snapclass source: persistentVolumeClaimName: www-web-0 Create the VolumeSnapshot object to request a snapshot. $ kubectl create -f dynamic-snapshot.yaml Verify the VolumeSnapshot object was created in Kubernetes. $ kubectl describe volumesnapshot www-web-snap-1 Name: www-web-snap-1 Namespace: default Labels: <none> Annotations: <none> API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Metadata: Creation Timestamp: 2020-06-12T19:00:42Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection snapshot.storage.kubernetes.io/volumesnapshot-bound-protection Generation: 1 Resource Version: 8840186 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/www-web-snap-1 UID: 7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Spec: Source: Persistent Volume Claim Name: www-web-0 Volume Snapshot Class Name: hpe-simplivity-snapclass Status: Bound Volume Snapshot Content Name: snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Creation Time: 2020-06-12T19:01:08Z Ready To Use: true Restore Size: 1Gi Events: <none> You can also verify the backup from HPE SimpliVity. $ svt-backup-show --pv pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd --datastore svt-ds .----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------. | Backups for PV 'pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd' on datastore 'svt-ds' | +-----------------------------------------------+--------+-------------+----------------------+------------+------------+----------------+-----------+----------+------+-------------+---------+ | Backup | Backup | Consistency | Backup | Expiration | Datacenter | Cluster Or | Status | Size | Sent | Replication | Family | | Name | Type | Type | Time | Time | | External Store | | | | End Time | | +-----------------------------------------------+--------+-------------+----------------------+------------+------------+----------------+-----------+----------+------+-------------+---------+ | snapshot-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc | Manual | None | 2020-Jun-12 19:01:08 | N/A | K8DC1 | K8Cluster1 | Protected | 656.00KB | 0B | N/A | vSphere | '-----------------------------------------------+--------+-------------+----------------------+------------+------------+----------------+-----------+----------+------+-------------+---------' Static Volume Snapshot Static volume snapshot provisioning allows cluster administrators to make existing snapshots available to a cluster. A common HPE SimpliVity use case is creating a snapshot from a scheduled backup. Static Snapshot Procedure Use the HPE SimpliVity svt-backup-show CLI to retrieve the ID of the backup that you wish to restore. $ svt-backup-show --pv pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd --datastore svt-ds --backup 2020-06-15T11:10:00-04:00 --output xml <CommandResult> <Backup> <dsRemoved></dsRemoved> <datacenter>K8DC1</datacenter> <expirationTime>1592320200</expirationTime> <replicationStartTime>0</replicationStartTime> <datacenterName>K8DC1</datacenterName> <partial></partial> <associatedCluster>00000000-0000-0000-0000-000000000000</associatedCluster> <zoneStatus>1</zoneStatus> <computeClusterName>K8Cluster1</computeClusterName> <consistent></consistent> <lastTimeSizeCalc>0</lastTimeSizeCalc> <treeId>e8f03589-bdf3-4cc4-b401-e00c05c8ad11</treeId> <percentComp>0</percentComp> <uniqueSize>0</uniqueSize> <percentTrans>0</percentTrans> <dcId>e871a438-bcfc-476b-9081-7042a466771c</dcId> <sentSize>0</sentSize> <timestamp>1592233800</timestamp> <repTaskId>00000000-0000-0000-0000-000000000000</repTaskId> <computeClusterHmsId>9168aa17-f5a7-4d3c-94f6-5f7293bb80c3:ClusterComputeResource:domain-c7</computeClusterHmsId> <datacenterHmsId>9168aa17-f5a7-4d3c-94f6-5f7293bb80c3:Datacenter:datacenter-2</datacenterHmsId> <dsId>833a131c-09f2-46a3-94a9-e4b4342a9efe</dsId> <datastore>svt-ds</datastore> <hypervisorType>1</hypervisorType> <replicationElapsedSeconds>0</replicationElapsedSeconds> <externalStoreName></externalStoreName> <pedigree>1</pedigree> <omnistackClusterId>e871a438-bcfc-476b-9081-7042a466771c</omnistackClusterId> <logicalSize>671744</logicalSize> <consistency>0</consistency> <replicationAttempts>0</replicationAttempts> <hiveId>559273a8-f909-477d-8ca9-c6707b9208c3</hiveId> <name>2020-06-15T11:10:00-04:00</name> <replicationEndTime>0</replicationEndTime> <id>3edb760f-d9ad-4975-b8e4-0e1bc0394644</id> <hiveName>pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd</hiveName> <backupId>3edb760f-d9ad-4975-b8e4-0e1bc0394644</backupId> <consistencyType>2</consistencyType> <replicaSet>c59a1042-3e83-d0c9-c28a-71bf95b68d64</replicaSet> <state>4</state> <hiveVolumeId>559273a8-f909-477d-8ca9-c6707b9208c3</hiveVolumeId> <backupStoreType>0</backupStoreType> </Backup> </CommandResult> Define a VolumeSnapshotContent object representing the pre-existing snapshot. The VolumeSnapshotContent name is a unique name of your choosing to represent the object in Kubernetes. The deletionPolicy enables you to configure what happens to a VolumeSnapshotContent object when the VolumeSnapshot object to which it is bound is deleted. The deletionPolicy of a volume snapshot can either be Retain or Delete . The snapshotHandle is the backup ID you retrieved in step 1. The volume snapshot reference name should be set to a unique name that you will set on the VolumeSnapshot object that will be bound to this VolumeSnapshotContent. apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotContent metadata: name: static-snap-content spec: deletionPolicy: Delete driver: csi.simplivity.hpe.com source: snapshotHandle: 3edb760f-d9ad-4975-b8e4-0e1bc0394644 volumeSnapshotRef: name: static-snap-example namespace: default Create the VolumeSnapshotContent object. $ kubectl create -f static-snapshot-content.yaml Define a VolumeSnapshot object. The name should match the volume snapshot reference name from the VolumeSnapshotContent object created in step 3. volumeSnapshotContentName specifies the name of the VolumeSnapshotContent object created in step 3. kind: VolumeSnapshot metadata: name: static-snap-example spec: source: volumeSnapshotContentName: static-snap-content Create the VolumeSnapshot object to bind the snapshot. $ kubectl create -f static-snapshot.yaml Verify the VolumeSnapshot object was created in Kubernetes. $ kubectl describe volumesnapshot static-snap-example Name: static-snap-example Namespace: default Labels: <none> Annotations: <none> API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Metadata: Creation Timestamp: 2020-06-15T16:07:22Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection snapshot.storage.kubernetes.io/volumesnapshot-bound-protection Generation: 1 Resource Version: 9789090 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/static-snap-example UID: f90e5b74-cf68-40f2-bb28-95e85944d56b Spec: Source: Volume Snapshot Content Name: static-snap-content Status: Bound Volume Snapshot Content Name: static-snap-content Creation Time: 2020-06-15T15:10:00Z Ready To Use: true Restore Size: 0 Events: <none> Restoring a Volume Snapshot Once the VolumeSnapshot object is bound and ready to use, you can restore the volume using the VolumeSnapshotContent object. To do so, create a persistent volume claim which specifies the volume snapshot object as the volume source. Snapshot Restore Procedure Define a PersistentVolumeClaim. The dataSource allows you to specify the snapshot being restored. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: www-web-restore-1 spec: storageClassName: svt-sc dataSource: name: www-web-snap-1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 1Gi Create the PVC object. $ kubectl create -f snap-restore.yaml persistentvolumeclaim/www-web-restore-1 created Verify the PVC was created successfully. $ kubectl describe pvc www-web-restore-1 Name: www-web-restore-1 Namespace: default StorageClass: svt-sc Status: Bound Volume: pvc-72fb5f0e-5a93-44fd-a5fe-c82339093115 Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Gi Access Modes: RWO VolumeMode: Filesystem DataSource: APIGroup: snapshot.storage.k8s.io Kind: VolumeSnapshot Name: www-web-snap-1 Mounted By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 6m6s csi.simplivity.hpe.com_svt-csi-controller-5844ff4c8c-jgv9k_2164c1a6-89af-4837-b6b6-c10015aa07c6 External provisioner is provisioning volume for claim \"default/www-web-restore-1\" Normal ExternalProvisioning 5m13s (x5 over 6m5s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.simplivity.hpe.com\" or manually created by system administrator Normal ProvisioningSucceeded 5m10s csi.simplivity.hpe.com_svt-csi-controller-5844ff4c8c-jgv9k_2164c1a6-89af-4837-b6b6-c10015aa07c6 Successfully provisioned volume pvc-72fb5f0e-5a93-44fd-a5fe-c82339093115 Deleting a Volume Snapshot Snapshots can be deleted when no longer needed. Snapshot Delete Procedure The VolumeSnapshotContent object Deletion Policy determines whether or not the underlying VolumeSnapshotContent object will be deleted or not when the VolumeSnapshot is deleted. If it is set to Retain then both the VolumeSnapshot and VolumeSnapshotContent object must be deleted to fully delete the snapshot. In this case, it is set to Delete and thus we only need to remove the VolumeSnapshot object. $ kubectl describe volumesnapshotcontent snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Name: snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Namespace: Labels: <none> Annotations: <none> API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshotContent Metadata: Creation Timestamp: 2020-06-12T19:00:42Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshotcontent-bound-protection Generation: 1 Resource Version: 8840185 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/volumesnapshotcontents/snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc UID: 15f8f342-e9d9-4039-8c26-4ec321a8f60e Spec: Deletion Policy: Delete Driver: csi.simplivity.hpe.com Source: Volume Handle: 202797ef-40e3-41ba-a8b1-3850cfc93901 Volume Snapshot Class Name: hpe-simplivity-snapclass Volume Snapshot Ref: API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Name: www-web-snap-1 Namespace: default Resource Version: 8840043 UID: 7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Status: Creation Time: 1591988468000000000 Ready To Use: true Restore Size: 1073741824 Snapshot Handle: 996730a3-6005-4f4c-a758-5203892faa0b Events: <none> Delete the VolumeSnapshot object. $ kubectl delete volumesnapshot www-web-snap-1 volumesnapshot.snapshot.storage.k8s.io \"www-web-snap-1\" deleted Verify the snapshot and content are removed. $ kubectl get volumesnapshot www-web-snap-1 Error from server (NotFound): volumesnapshots.snapshot.storage.k8s.io \"www-web-snap-1\" not found $ kubectl get volumesnapshotcontent snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Error from server (NotFound): volumesnapshotcontents.snapshot.storage.k8s.io \"snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc\" not found Verify that the backup has been deleted from HPE SimpliVity. $ svt-backup-show --pv pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd --datastore svt-ds No backups found.","title":"Volume Snapshots"},{"location":"snapshot/#hpe-simplivity-csi-driver-volume-snapshots","text":"For dynamic volume creation, creating a PersistentVolumeClaim (PVC) initiates the creation of a PersistentVolume (PV) which contains the data. A PVC also specifies a StorageClass which provides additional attributes (e.g. SimpliVity datastore). The CSI snapshot feature follows the same pattern. Creating a VolumeSnapshot triggers creation of a VolumeSnapshotContent object which contains the snapshot data. A VolumeSnapshot also specifies a VolumeSnapshotClass to provide additional attributes (e.g. retention policy). VolumeSnapshotContent : The Kubernetes cluster resource that represents a snapshot of a persistent volume. VolumeSnapshot : A snapshot request. Creating a VolumeSnapshot triggers a snapshot (VolumeSnapshotContent) and the objects are bound together. There is a one-to-one mapping between VolumeSnapshot and VolumeSnapshotContent objects. VolumeSnapshotClass : Dynamically provisioned snapshots specify a VolumeSnapshotClass to provide additional parameters for the snapshot operation. This is similar to how a StorageClass is used for PVs, and allows volumes created from the same StorageClass to be snapshotted with different options (e.g. retention policy).","title":"HPE SimpliVity CSI Driver - Volume Snapshots"},{"location":"snapshot/#volume-snapshot-provisioning","text":"There are two types of snapshot provisioning in a Kubernetes cluster: Dynamic Volume Snapshot : Create an on-demand snapshot. Static Volume Snapshot : Create a snapshot for a pre-existing HPE SimpliVity snapshot. For example, to create a Kubernetes snapshot from a scheduled backup.","title":"Volume Snapshot Provisioning"},{"location":"snapshot/#dynamic-volume-snapshot","text":"Volume snapshots of HPE SimpliVity volumes can be created on demand using the Kubernetes CLI. To do so, create a VolumeSnapshot object containing the PersistentVolumeClaim that you wish to snap. Kubernetes does not quiesce IO during a snapshot operation and thus the snapshot will just be crash consistent. To take an application consistent snapshot, you must take additional steps (e.g. shutdown the pod consuming the PV).","title":"Dynamic Volume Snapshot"},{"location":"snapshot/#dynamic-snapshot-procedure","text":"Define a VolumeSnapshotClass. The deletionPolicy enables you to configure what happens to a VolumeSnapshotContent object when the VolumeSnapshot object to which it is bound is deleted. The deletionPolicy of a volume snapshot can either be Retain or Delete . apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: hpe-simplivity-snapclass driver: csi.simplivity.hpe.com deletionPolicy: Delete parameters: description: \"Snapshot created by the HPE SimpliVity vSphere CSI Driver\" Create the VolumeSnapshotClass object. $ kubectl create -f example-snapshotclass.yaml Define a VolumeSnapshot object. volumeSnapshotClassName specifies the snapshot options (defined in step 2) and persistentVolumeClaim identifies the snapshot source volume. apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: www-web-snap-1 spec: volumeSnapshotClassName: hpe-simplivity-snapclass source: persistentVolumeClaimName: www-web-0 Create the VolumeSnapshot object to request a snapshot. $ kubectl create -f dynamic-snapshot.yaml Verify the VolumeSnapshot object was created in Kubernetes. $ kubectl describe volumesnapshot www-web-snap-1 Name: www-web-snap-1 Namespace: default Labels: <none> Annotations: <none> API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Metadata: Creation Timestamp: 2020-06-12T19:00:42Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection snapshot.storage.kubernetes.io/volumesnapshot-bound-protection Generation: 1 Resource Version: 8840186 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/www-web-snap-1 UID: 7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Spec: Source: Persistent Volume Claim Name: www-web-0 Volume Snapshot Class Name: hpe-simplivity-snapclass Status: Bound Volume Snapshot Content Name: snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Creation Time: 2020-06-12T19:01:08Z Ready To Use: true Restore Size: 1Gi Events: <none> You can also verify the backup from HPE SimpliVity. $ svt-backup-show --pv pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd --datastore svt-ds .----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------. | Backups for PV 'pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd' on datastore 'svt-ds' | +-----------------------------------------------+--------+-------------+----------------------+------------+------------+----------------+-----------+----------+------+-------------+---------+ | Backup | Backup | Consistency | Backup | Expiration | Datacenter | Cluster Or | Status | Size | Sent | Replication | Family | | Name | Type | Type | Time | Time | | External Store | | | | End Time | | +-----------------------------------------------+--------+-------------+----------------------+------------+------------+----------------+-----------+----------+------+-------------+---------+ | snapshot-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc | Manual | None | 2020-Jun-12 19:01:08 | N/A | K8DC1 | K8Cluster1 | Protected | 656.00KB | 0B | N/A | vSphere | '-----------------------------------------------+--------+-------------+----------------------+------------+------------+----------------+-----------+----------+------+-------------+---------'","title":"Dynamic Snapshot Procedure"},{"location":"snapshot/#static-volume-snapshot","text":"Static volume snapshot provisioning allows cluster administrators to make existing snapshots available to a cluster. A common HPE SimpliVity use case is creating a snapshot from a scheduled backup.","title":"Static Volume Snapshot"},{"location":"snapshot/#static-snapshot-procedure","text":"Use the HPE SimpliVity svt-backup-show CLI to retrieve the ID of the backup that you wish to restore. $ svt-backup-show --pv pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd --datastore svt-ds --backup 2020-06-15T11:10:00-04:00 --output xml <CommandResult> <Backup> <dsRemoved></dsRemoved> <datacenter>K8DC1</datacenter> <expirationTime>1592320200</expirationTime> <replicationStartTime>0</replicationStartTime> <datacenterName>K8DC1</datacenterName> <partial></partial> <associatedCluster>00000000-0000-0000-0000-000000000000</associatedCluster> <zoneStatus>1</zoneStatus> <computeClusterName>K8Cluster1</computeClusterName> <consistent></consistent> <lastTimeSizeCalc>0</lastTimeSizeCalc> <treeId>e8f03589-bdf3-4cc4-b401-e00c05c8ad11</treeId> <percentComp>0</percentComp> <uniqueSize>0</uniqueSize> <percentTrans>0</percentTrans> <dcId>e871a438-bcfc-476b-9081-7042a466771c</dcId> <sentSize>0</sentSize> <timestamp>1592233800</timestamp> <repTaskId>00000000-0000-0000-0000-000000000000</repTaskId> <computeClusterHmsId>9168aa17-f5a7-4d3c-94f6-5f7293bb80c3:ClusterComputeResource:domain-c7</computeClusterHmsId> <datacenterHmsId>9168aa17-f5a7-4d3c-94f6-5f7293bb80c3:Datacenter:datacenter-2</datacenterHmsId> <dsId>833a131c-09f2-46a3-94a9-e4b4342a9efe</dsId> <datastore>svt-ds</datastore> <hypervisorType>1</hypervisorType> <replicationElapsedSeconds>0</replicationElapsedSeconds> <externalStoreName></externalStoreName> <pedigree>1</pedigree> <omnistackClusterId>e871a438-bcfc-476b-9081-7042a466771c</omnistackClusterId> <logicalSize>671744</logicalSize> <consistency>0</consistency> <replicationAttempts>0</replicationAttempts> <hiveId>559273a8-f909-477d-8ca9-c6707b9208c3</hiveId> <name>2020-06-15T11:10:00-04:00</name> <replicationEndTime>0</replicationEndTime> <id>3edb760f-d9ad-4975-b8e4-0e1bc0394644</id> <hiveName>pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd</hiveName> <backupId>3edb760f-d9ad-4975-b8e4-0e1bc0394644</backupId> <consistencyType>2</consistencyType> <replicaSet>c59a1042-3e83-d0c9-c28a-71bf95b68d64</replicaSet> <state>4</state> <hiveVolumeId>559273a8-f909-477d-8ca9-c6707b9208c3</hiveVolumeId> <backupStoreType>0</backupStoreType> </Backup> </CommandResult> Define a VolumeSnapshotContent object representing the pre-existing snapshot. The VolumeSnapshotContent name is a unique name of your choosing to represent the object in Kubernetes. The deletionPolicy enables you to configure what happens to a VolumeSnapshotContent object when the VolumeSnapshot object to which it is bound is deleted. The deletionPolicy of a volume snapshot can either be Retain or Delete . The snapshotHandle is the backup ID you retrieved in step 1. The volume snapshot reference name should be set to a unique name that you will set on the VolumeSnapshot object that will be bound to this VolumeSnapshotContent. apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotContent metadata: name: static-snap-content spec: deletionPolicy: Delete driver: csi.simplivity.hpe.com source: snapshotHandle: 3edb760f-d9ad-4975-b8e4-0e1bc0394644 volumeSnapshotRef: name: static-snap-example namespace: default Create the VolumeSnapshotContent object. $ kubectl create -f static-snapshot-content.yaml Define a VolumeSnapshot object. The name should match the volume snapshot reference name from the VolumeSnapshotContent object created in step 3. volumeSnapshotContentName specifies the name of the VolumeSnapshotContent object created in step 3. kind: VolumeSnapshot metadata: name: static-snap-example spec: source: volumeSnapshotContentName: static-snap-content Create the VolumeSnapshot object to bind the snapshot. $ kubectl create -f static-snapshot.yaml Verify the VolumeSnapshot object was created in Kubernetes. $ kubectl describe volumesnapshot static-snap-example Name: static-snap-example Namespace: default Labels: <none> Annotations: <none> API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Metadata: Creation Timestamp: 2020-06-15T16:07:22Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshot-as-source-protection snapshot.storage.kubernetes.io/volumesnapshot-bound-protection Generation: 1 Resource Version: 9789090 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/namespaces/default/volumesnapshots/static-snap-example UID: f90e5b74-cf68-40f2-bb28-95e85944d56b Spec: Source: Volume Snapshot Content Name: static-snap-content Status: Bound Volume Snapshot Content Name: static-snap-content Creation Time: 2020-06-15T15:10:00Z Ready To Use: true Restore Size: 0 Events: <none>","title":"Static Snapshot Procedure"},{"location":"snapshot/#restoring-a-volume-snapshot","text":"Once the VolumeSnapshot object is bound and ready to use, you can restore the volume using the VolumeSnapshotContent object. To do so, create a persistent volume claim which specifies the volume snapshot object as the volume source.","title":"Restoring a Volume Snapshot"},{"location":"snapshot/#snapshot-restore-procedure","text":"Define a PersistentVolumeClaim. The dataSource allows you to specify the snapshot being restored. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: www-web-restore-1 spec: storageClassName: svt-sc dataSource: name: www-web-snap-1 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 1Gi Create the PVC object. $ kubectl create -f snap-restore.yaml persistentvolumeclaim/www-web-restore-1 created Verify the PVC was created successfully. $ kubectl describe pvc www-web-restore-1 Name: www-web-restore-1 Namespace: default StorageClass: svt-sc Status: Bound Volume: pvc-72fb5f0e-5a93-44fd-a5fe-c82339093115 Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 1Gi Access Modes: RWO VolumeMode: Filesystem DataSource: APIGroup: snapshot.storage.k8s.io Kind: VolumeSnapshot Name: www-web-snap-1 Mounted By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 6m6s csi.simplivity.hpe.com_svt-csi-controller-5844ff4c8c-jgv9k_2164c1a6-89af-4837-b6b6-c10015aa07c6 External provisioner is provisioning volume for claim \"default/www-web-restore-1\" Normal ExternalProvisioning 5m13s (x5 over 6m5s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.simplivity.hpe.com\" or manually created by system administrator Normal ProvisioningSucceeded 5m10s csi.simplivity.hpe.com_svt-csi-controller-5844ff4c8c-jgv9k_2164c1a6-89af-4837-b6b6-c10015aa07c6 Successfully provisioned volume pvc-72fb5f0e-5a93-44fd-a5fe-c82339093115","title":"Snapshot Restore Procedure"},{"location":"snapshot/#deleting-a-volume-snapshot","text":"Snapshots can be deleted when no longer needed.","title":"Deleting a Volume Snapshot"},{"location":"snapshot/#snapshot-delete-procedure","text":"The VolumeSnapshotContent object Deletion Policy determines whether or not the underlying VolumeSnapshotContent object will be deleted or not when the VolumeSnapshot is deleted. If it is set to Retain then both the VolumeSnapshot and VolumeSnapshotContent object must be deleted to fully delete the snapshot. In this case, it is set to Delete and thus we only need to remove the VolumeSnapshot object. $ kubectl describe volumesnapshotcontent snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Name: snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Namespace: Labels: <none> Annotations: <none> API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshotContent Metadata: Creation Timestamp: 2020-06-12T19:00:42Z Finalizers: snapshot.storage.kubernetes.io/volumesnapshotcontent-bound-protection Generation: 1 Resource Version: 8840185 Self Link: /apis/snapshot.storage.k8s.io/v1beta1/volumesnapshotcontents/snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc UID: 15f8f342-e9d9-4039-8c26-4ec321a8f60e Spec: Deletion Policy: Delete Driver: csi.simplivity.hpe.com Source: Volume Handle: 202797ef-40e3-41ba-a8b1-3850cfc93901 Volume Snapshot Class Name: hpe-simplivity-snapclass Volume Snapshot Ref: API Version: snapshot.storage.k8s.io/v1beta1 Kind: VolumeSnapshot Name: www-web-snap-1 Namespace: default Resource Version: 8840043 UID: 7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Status: Creation Time: 1591988468000000000 Ready To Use: true Restore Size: 1073741824 Snapshot Handle: 996730a3-6005-4f4c-a758-5203892faa0b Events: <none> Delete the VolumeSnapshot object. $ kubectl delete volumesnapshot www-web-snap-1 volumesnapshot.snapshot.storage.k8s.io \"www-web-snap-1\" deleted Verify the snapshot and content are removed. $ kubectl get volumesnapshot www-web-snap-1 Error from server (NotFound): volumesnapshots.snapshot.storage.k8s.io \"www-web-snap-1\" not found $ kubectl get volumesnapshotcontent snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc Error from server (NotFound): volumesnapshotcontents.snapshot.storage.k8s.io \"snapcontent-7fa13b4b-f370-40c4-9946-4c7e2c72dcdc\" not found Verify that the backup has been deleted from HPE SimpliVity. $ svt-backup-show --pv pvc-e3fe7f3b-e00a-4b0d-9d65-89c3564081be_fcd --datastore svt-ds No backups found.","title":"Snapshot Delete Procedure"},{"location":"support-information/","text":"Support Information Limits Consult the HPE OmniStack documentation for VM limits, which applies to persistent volumes (PV) as well. The limits may be more restrictive depending on the hardware configuration. Note: Only a single vCenter is supported by HPE SimpliVity CSI Driver for vSphere. Make sure Kubernetes node VMs do not spread across multiple vCenter servers. Compatibility Matrix HPE OmniStack for vSphere Compatibility The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with vSphere releases. HPE SimpliVity CSI Versions HPE OmniStack Version 1.0.0 4.1.0 2.0.0 4.1.0, 4.1.0U1 vSphere/ESXi Compatibility The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with vSphere releases. HPE SimpliVity CSI Versions vSphere/ESXi Version 1.0.0 6.7U3 2.0.0 7.0 Kubernetes Compatibility The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with Kubernetes releases. HPE SimpliVity CSI Versions Kubernetes Versions 1.0.0 1.17, 1.18 2.0.0 1.20 vSphere CPI Compatibility The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with Kubernetes releases. HPE SimpliVity CSI Versions vSphere CPI Versions 1.0.0 1.1.0 2.0.0 1.20.0 CSI Snapshot Controller Compatibility The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with Kubernetes releases. HPE SimpliVity CSI Versions CSI Snapshot Controller Versions 1.0.0 2.1.1 2.0.0 4.0.0 Features Feature Supported Datastores supported SimpliVity datastores only Static Provisioning Yes Dynamic Provisioning Yes Access mode RWO Volume Topology/Zones Yes Snapshots Yes Encryption No Offline Volume Expansion Yes (v2.0.0 onwards) Additional Support Information To access documentation and support services, visit the Hewlett Packard Enterprise Support Center .","title":"Support Information"},{"location":"support-information/#support-information","text":"","title":"Support Information"},{"location":"support-information/#limits","text":"Consult the HPE OmniStack documentation for VM limits, which applies to persistent volumes (PV) as well. The limits may be more restrictive depending on the hardware configuration. Note: Only a single vCenter is supported by HPE SimpliVity CSI Driver for vSphere. Make sure Kubernetes node VMs do not spread across multiple vCenter servers.","title":"Limits"},{"location":"support-information/#compatibility-matrix","text":"","title":"Compatibility Matrix "},{"location":"support-information/#hpe-omnistack-for-vsphere-compatibility","text":"The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with vSphere releases. HPE SimpliVity CSI Versions HPE OmniStack Version 1.0.0 4.1.0 2.0.0 4.1.0, 4.1.0U1","title":"HPE OmniStack for vSphere Compatibility"},{"location":"support-information/#vsphereesxi-compatibility","text":"The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with vSphere releases. HPE SimpliVity CSI Versions vSphere/ESXi Version 1.0.0 6.7U3 2.0.0 7.0","title":"vSphere/ESXi Compatibility"},{"location":"support-information/#kubernetes-compatibility","text":"The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with Kubernetes releases. HPE SimpliVity CSI Versions Kubernetes Versions 1.0.0 1.17, 1.18 2.0.0 1.20","title":"Kubernetes Compatibility"},{"location":"support-information/#vsphere-cpi-compatibility","text":"The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with Kubernetes releases. HPE SimpliVity CSI Versions vSphere CPI Versions 1.0.0 1.1.0 2.0.0 1.20.0","title":"vSphere CPI Compatibility"},{"location":"support-information/#csi-snapshot-controller-compatibility","text":"The following table describes compatibility of the HPE SimpliVity CSI driver for vSphere releases with Kubernetes releases. HPE SimpliVity CSI Versions CSI Snapshot Controller Versions 1.0.0 2.1.1 2.0.0 4.0.0","title":"CSI Snapshot Controller Compatibility"},{"location":"support-information/#features","text":"Feature Supported Datastores supported SimpliVity datastores only Static Provisioning Yes Dynamic Provisioning Yes Access mode RWO Volume Topology/Zones Yes Snapshots Yes Encryption No Offline Volume Expansion Yes (v2.0.0 onwards)","title":"Features"},{"location":"support-information/#additional-support-information","text":"To access documentation and support services, visit the Hewlett Packard Enterprise Support Center .","title":"Additional Support Information"},{"location":"using-topologies/","text":"Volume Topology for HPE SimpliVity CSI Driver for vSphere Prerequisite : Enable topology in the Kubernetes Cluster. Follow steps mentioned in Configuring Topologies . HPE SimpliVity datastores are only accessible to the hosts that form the local cluster. This is fine if the Kubernetes cluster nodes are all contained on a single HPE SimpliVity/ESXi cluster. However, there may be a need to create a Kubernetes cluster that spans multiple HPE SimpliVity clusters to provide additional fault tolerance or simply to create really large Kubernetes clusters. In this case, worker nodes will only have access to datastores hosted by the local HPE SimpliVity cluster. So how will Kubernetes know which persistent volumes are accessible from which nodes to be able to intelligently provision storage and recover from faults? The topology aware provisioning feature was added to Kubernetes to handle this scenario. Cloud providers (vSphere in this case) provide region and zone information to Kubernetes so that volumes will get provisioned in an appropriate zone that can run a pod allowing for an ease of deployment and scale of stateful workloads across failure domains to provide high availability and fault tolerance. For additional information see Kubernetes best practices for running in multiple zones . Deploy workloads using topology with immediate volume binding mode Deploy workloads using topology with WaitForFirstConsumer volume binding mode Immediate volume binding mode When topology is enabled in the cluster, you can deploy a Kubernetes workload to a specific region or zone defined in the topology. Use the sample workflow to provision and verify your workloads. Create a StorageClass that defines zone and region mapping. To the StorageClass YAML file, add zone-a and region-1 in the allowedTopologies field. For datastoreurl specify a datastore that is accessible to all nodes in zone-a. $ tee example-zone-sc.yaml >/dev/null <<'EOF' kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-zone-sc provisioner: csi.simplivity.hpe.com parameters: datastoreurl: ds:///vmfs/volumes/0e65b0b0-00cd0c66/ allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - zone-a - key: failure-domain.beta.kubernetes.io/region values: - region-1 EOF Note: Here volumeBindingMode will be Immediate , as it is default when not specified. $ kubectl create -f example-zone-sc.yaml storageclass.storage.k8s.io/example-zone-sc created Create a PersistenceVolumeClaim. $ tee example-zone-pvc.yaml >/dev/null <<'EOF' apiVersion: v1 kind: PersistentVolumeClaim metadata: name: example-zone-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: example-zone-sc EOF $ kubectl create -f example-zone-pvc.yaml persistentvolumeclaim/example-zone-pvc created Verify that a volume is created for the PersistentVolumeClaim. $ kubectl get pvc example-zone-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-zone-pvc Bound pvc-4f98579d-2550-4cf2-98bc-dbd117c5906f 5Gi RWO example-zone-sc 76s Verify that the persistent volume is provisioned with the Node Affinity rules containing zone and region specified in the StorageClass. $ kubectl describe pv pvc-4f98579d-2550-4cf2-98bc-dbd117c5906f Name: pvc-4f98579d-2550-4cf2-98bc-dbd117c5906f Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: example-zone-sc Status: Bound Claim: default/example-zone-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/zone in [zone-a] failure-domain.beta.kubernetes.io/region in [region-1] Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.simplivity.hpe.com VolumeHandle: ffc618b7-75a6-4183-ae2a-a5a590dfe3b8 ReadOnly: false VolumeAttributes: datastoreurl=ds:///vmfs/volumes/0e65b0b0-00cd0c66/ storage.kubernetes.io/csiProvisionerIdentity=1589983635272-8081-csi.simplivity.hpe.com type=HPE SimpliVity CNS Block Volume Events: <none> Create a pod. $ tee example-zone-pod.yaml >/dev/null <<'EOF' apiVersion: v1 kind: Pod metadata: name: example-zone-pod spec: containers: - name: test-container image: gcr.io/google_containers/busybox:1.24 command: [\"/bin/sh\", \"-c\", \"echo 'hello' > /mnt/volume1/index.html && chmod o+rX /mnt /mnt/volume1/index.html && while true ; do sleep 2 ; done\"] volumeMounts: - name: test-volume mountPath: /mnt/volume1 restartPolicy: Never volumes: - name: test-volume persistentVolumeClaim: claimName: example-zone-pvc EOF $ kubectl create -f example-zone-pod.yaml pod/example-zone-pod created Pod is scheduled on the node k8s-node1 which belongs to zone: \"zone-a\" and region: \"region-1\" $ kubectl describe pod example-zone-pod | egrep \"Node:\" Node: k8s-r1za-w01/10.74.48.26 WaitForFirstConsumer volume binding mode The HPE SimpliVity CSI Driver for vSphere supports topology-aware volume provisioning with WaitForFirstConsumer Topology-aware provisioning allows Kubernetes to make intelligent decisions and find the best place to dynamically provision a volume for a pod. In multi-zone clusters, volumes are provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance. external-provisioner must be deployed with the --strict-topology arguments. This argument controls which topology information is passed to CreateVolumeRequest.AccessibilityRequirements in case of a delayed binding. For information on how this option changes the result, see this table . This option has no effect if the topology feature is disabled or the immediate volume binding mode is used. Create a StorageClass with the volumeBindingMode parameter set to WaitForFirstConsumer . For this example, a storagepolicyname is specified that contains datastores from all three zones (zone-a, zone-b, zone-c). $ tee topology-aware-standard.yaml >/dev/null <<'EOF' apiVersion: v1 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: topology-aware-standard provisioner: csi.simplivity.hpe.com volumeBindingMode: WaitForFirstConsumer parameters: storagepolicyname: \"SvtGoldPolicy\" EOF $ kubectl create -f topology-aware-standard.yaml storageclass.storage.k8s.io/topology-aware-standard created This new setting instructs the volume provisioner, instead of creating a volume immediately, to wait until a pod using an associated PVC runs through scheduling. Note that in the previous StorageClass, failure-domain.beta.kubernetes.io/zone and failure-domain.beta.kubernetes.io/region were specified in the allowedTopologies entry. You do not need to specify them again, as pod policies now drive the decision of which zone to use for a volume provisioning. Create a pod and PVC using the StorageClass created previously. The following example demonstrates multiple pod constraints and scheduling policies. $ tee topology-aware-statefulset.yaml >/dev/null <<'EOF' --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: replicas: 2 selector: matchLabels: app: nginx serviceName: nginx template: metadata: labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: failure-domain.beta.kubernetes.io/zone operator: In values: - zone-a - zone-b podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: failure-domain.beta.kubernetes.io/zone containers: - name: nginx image: gcr.io/google_containers/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html - name: logs mountPath: /logs volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: topology-aware-standard resources: requests: storage: 5Gi - metadata: name: logs spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: topology-aware-standard resources: requests: storage: 1Gi EOF $ kubectl create -f topology-aware-statefulset.yaml statefulset.apps/web created Verify statefulset is up and running. $ kubectl get statefulset NAME READY AGE web 2/2 9m8s Review your pods and your nodes. Pods are created in zone-a and zone-b specified in the nodeAffinity entry. web-0 is scheduled on the node k8s-r1zb-w01 , which belongs to zone-b . web-1 is scheduled on the node k8s-r1za-w02 , which belongs to zone-a . $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-0 1/1 Running 0 17m 10.233.125.2 k8s-r1zb-w01 <none> <none> web-1 1/1 Running 0 13m 10.233.99.3 k8s-r1za-w02 <none> <none> $ kubectl get nodes k8s-r1zb-w01 k8s-r1za-w02 -L failure-domain.beta.kubernetes.io/zone -L failure-domain.beta.kubernetes.io/region --no-headers k8s-r1zb-w01 Ready <none> 16h v1.17.5 zone-b region-1 k8s-r1za-w02 Ready <none> 16h v1.17.5 zone-a region-1 Verify volumes are provisioned in zones according to the policies set by the pod. $ kubectl describe pvc www-web-0 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1zb-w01 $ kubectl describe pvc logs-web-0 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1zb-w01 $ kubectl describe pvc www-web-1 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1za-w02 $ kubectl describe pvc logs-web-1 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1za-w02 $ kubectl get pv -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.claimRef.name}{\"\\t\"}{.spec.nodeAffinity}{\"\\n\"}{end}' pvc-359c8f41-f60a-4538-bd52-e36d6fe2aa54 logs-web-1 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-a]] map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]]]]]]] pvc-c0cf4ed1-c4a7-4499-a88b-4153e9a0f461 www-web-1 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-a]] map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]]]]]]] pvc-c54feadd-af93-44de-87a4-afc5d92e9054 www-web-0 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]] map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-b]]]]]]] pvc-de8e825c-cd87-4865-968c-324672da0c6d logs-web-0 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-b]] map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]]]]]]] If required, specify allowedTopologies. When a cluster operator specifies the WaitForFirstConsumer volume binding mode, it is no longer necessary to restrict provisioning to specific topologies in most situations. However, if required, you can specify allowedTopologies. The following example demonstrates how to restrict the topology to specific zone. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-sc provisioner: csi.simplivity.hpe.com volumeBindingMode: WaitForFirstConsumer parameters: datastoreurl: ds:///vmfs/volumes/0e65b0b0-00cd0c66/ allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - zone-b - key: failure-domain.beta.kubernetes.io/region values: - region-1","title":"Using Topologies"},{"location":"using-topologies/#volume-topology-for-hpe-simplivity-csi-driver-for-vsphere","text":"Prerequisite : Enable topology in the Kubernetes Cluster. Follow steps mentioned in Configuring Topologies . HPE SimpliVity datastores are only accessible to the hosts that form the local cluster. This is fine if the Kubernetes cluster nodes are all contained on a single HPE SimpliVity/ESXi cluster. However, there may be a need to create a Kubernetes cluster that spans multiple HPE SimpliVity clusters to provide additional fault tolerance or simply to create really large Kubernetes clusters. In this case, worker nodes will only have access to datastores hosted by the local HPE SimpliVity cluster. So how will Kubernetes know which persistent volumes are accessible from which nodes to be able to intelligently provision storage and recover from faults? The topology aware provisioning feature was added to Kubernetes to handle this scenario. Cloud providers (vSphere in this case) provide region and zone information to Kubernetes so that volumes will get provisioned in an appropriate zone that can run a pod allowing for an ease of deployment and scale of stateful workloads across failure domains to provide high availability and fault tolerance. For additional information see Kubernetes best practices for running in multiple zones . Deploy workloads using topology with immediate volume binding mode Deploy workloads using topology with WaitForFirstConsumer volume binding mode","title":"Volume Topology for HPE SimpliVity CSI Driver for vSphere"},{"location":"using-topologies/#immediate-volume-binding-mode","text":"When topology is enabled in the cluster, you can deploy a Kubernetes workload to a specific region or zone defined in the topology. Use the sample workflow to provision and verify your workloads. Create a StorageClass that defines zone and region mapping. To the StorageClass YAML file, add zone-a and region-1 in the allowedTopologies field. For datastoreurl specify a datastore that is accessible to all nodes in zone-a. $ tee example-zone-sc.yaml >/dev/null <<'EOF' kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-zone-sc provisioner: csi.simplivity.hpe.com parameters: datastoreurl: ds:///vmfs/volumes/0e65b0b0-00cd0c66/ allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - zone-a - key: failure-domain.beta.kubernetes.io/region values: - region-1 EOF Note: Here volumeBindingMode will be Immediate , as it is default when not specified. $ kubectl create -f example-zone-sc.yaml storageclass.storage.k8s.io/example-zone-sc created Create a PersistenceVolumeClaim. $ tee example-zone-pvc.yaml >/dev/null <<'EOF' apiVersion: v1 kind: PersistentVolumeClaim metadata: name: example-zone-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: example-zone-sc EOF $ kubectl create -f example-zone-pvc.yaml persistentvolumeclaim/example-zone-pvc created Verify that a volume is created for the PersistentVolumeClaim. $ kubectl get pvc example-zone-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-zone-pvc Bound pvc-4f98579d-2550-4cf2-98bc-dbd117c5906f 5Gi RWO example-zone-sc 76s Verify that the persistent volume is provisioned with the Node Affinity rules containing zone and region specified in the StorageClass. $ kubectl describe pv pvc-4f98579d-2550-4cf2-98bc-dbd117c5906f Name: pvc-4f98579d-2550-4cf2-98bc-dbd117c5906f Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: example-zone-sc Status: Bound Claim: default/example-zone-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/zone in [zone-a] failure-domain.beta.kubernetes.io/region in [region-1] Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.simplivity.hpe.com VolumeHandle: ffc618b7-75a6-4183-ae2a-a5a590dfe3b8 ReadOnly: false VolumeAttributes: datastoreurl=ds:///vmfs/volumes/0e65b0b0-00cd0c66/ storage.kubernetes.io/csiProvisionerIdentity=1589983635272-8081-csi.simplivity.hpe.com type=HPE SimpliVity CNS Block Volume Events: <none> Create a pod. $ tee example-zone-pod.yaml >/dev/null <<'EOF' apiVersion: v1 kind: Pod metadata: name: example-zone-pod spec: containers: - name: test-container image: gcr.io/google_containers/busybox:1.24 command: [\"/bin/sh\", \"-c\", \"echo 'hello' > /mnt/volume1/index.html && chmod o+rX /mnt /mnt/volume1/index.html && while true ; do sleep 2 ; done\"] volumeMounts: - name: test-volume mountPath: /mnt/volume1 restartPolicy: Never volumes: - name: test-volume persistentVolumeClaim: claimName: example-zone-pvc EOF $ kubectl create -f example-zone-pod.yaml pod/example-zone-pod created Pod is scheduled on the node k8s-node1 which belongs to zone: \"zone-a\" and region: \"region-1\" $ kubectl describe pod example-zone-pod | egrep \"Node:\" Node: k8s-r1za-w01/10.74.48.26","title":"Immediate volume binding mode "},{"location":"using-topologies/#waitforfirstconsumer-volume-binding-mode","text":"The HPE SimpliVity CSI Driver for vSphere supports topology-aware volume provisioning with WaitForFirstConsumer Topology-aware provisioning allows Kubernetes to make intelligent decisions and find the best place to dynamically provision a volume for a pod. In multi-zone clusters, volumes are provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance. external-provisioner must be deployed with the --strict-topology arguments. This argument controls which topology information is passed to CreateVolumeRequest.AccessibilityRequirements in case of a delayed binding. For information on how this option changes the result, see this table . This option has no effect if the topology feature is disabled or the immediate volume binding mode is used. Create a StorageClass with the volumeBindingMode parameter set to WaitForFirstConsumer . For this example, a storagepolicyname is specified that contains datastores from all three zones (zone-a, zone-b, zone-c). $ tee topology-aware-standard.yaml >/dev/null <<'EOF' apiVersion: v1 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: topology-aware-standard provisioner: csi.simplivity.hpe.com volumeBindingMode: WaitForFirstConsumer parameters: storagepolicyname: \"SvtGoldPolicy\" EOF $ kubectl create -f topology-aware-standard.yaml storageclass.storage.k8s.io/topology-aware-standard created This new setting instructs the volume provisioner, instead of creating a volume immediately, to wait until a pod using an associated PVC runs through scheduling. Note that in the previous StorageClass, failure-domain.beta.kubernetes.io/zone and failure-domain.beta.kubernetes.io/region were specified in the allowedTopologies entry. You do not need to specify them again, as pod policies now drive the decision of which zone to use for a volume provisioning. Create a pod and PVC using the StorageClass created previously. The following example demonstrates multiple pod constraints and scheduling policies. $ tee topology-aware-statefulset.yaml >/dev/null <<'EOF' --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: replicas: 2 selector: matchLabels: app: nginx serviceName: nginx template: metadata: labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: failure-domain.beta.kubernetes.io/zone operator: In values: - zone-a - zone-b podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: failure-domain.beta.kubernetes.io/zone containers: - name: nginx image: gcr.io/google_containers/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html - name: logs mountPath: /logs volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: topology-aware-standard resources: requests: storage: 5Gi - metadata: name: logs spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: topology-aware-standard resources: requests: storage: 1Gi EOF $ kubectl create -f topology-aware-statefulset.yaml statefulset.apps/web created Verify statefulset is up and running. $ kubectl get statefulset NAME READY AGE web 2/2 9m8s Review your pods and your nodes. Pods are created in zone-a and zone-b specified in the nodeAffinity entry. web-0 is scheduled on the node k8s-r1zb-w01 , which belongs to zone-b . web-1 is scheduled on the node k8s-r1za-w02 , which belongs to zone-a . $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-0 1/1 Running 0 17m 10.233.125.2 k8s-r1zb-w01 <none> <none> web-1 1/1 Running 0 13m 10.233.99.3 k8s-r1za-w02 <none> <none> $ kubectl get nodes k8s-r1zb-w01 k8s-r1za-w02 -L failure-domain.beta.kubernetes.io/zone -L failure-domain.beta.kubernetes.io/region --no-headers k8s-r1zb-w01 Ready <none> 16h v1.17.5 zone-b region-1 k8s-r1za-w02 Ready <none> 16h v1.17.5 zone-a region-1 Verify volumes are provisioned in zones according to the policies set by the pod. $ kubectl describe pvc www-web-0 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1zb-w01 $ kubectl describe pvc logs-web-0 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1zb-w01 $ kubectl describe pvc www-web-1 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1za-w02 $ kubectl describe pvc logs-web-1 | egrep \"volume.kubernetes.io/selected-node\" volume.kubernetes.io/selected-node: k8s-r1za-w02 $ kubectl get pv -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.claimRef.name}{\"\\t\"}{.spec.nodeAffinity}{\"\\n\"}{end}' pvc-359c8f41-f60a-4538-bd52-e36d6fe2aa54 logs-web-1 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-a]] map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]]]]]]] pvc-c0cf4ed1-c4a7-4499-a88b-4153e9a0f461 www-web-1 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-a]] map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]]]]]]] pvc-c54feadd-af93-44de-87a4-afc5d92e9054 www-web-0 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]] map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-b]]]]]]] pvc-de8e825c-cd87-4865-968c-324672da0c6d logs-web-0 map[required:map[nodeSelectorTerms:[map[matchExpressions:[map[key:failure-domain.beta.kubernetes.io/zone operator:In values:[zone-b]] map[key:failure-domain.beta.kubernetes.io/region operator:In values:[region-1]]]]]]] If required, specify allowedTopologies. When a cluster operator specifies the WaitForFirstConsumer volume binding mode, it is no longer necessary to restrict provisioning to specific topologies in most situations. However, if required, you can specify allowedTopologies. The following example demonstrates how to restrict the topology to specific zone. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-sc provisioner: csi.simplivity.hpe.com volumeBindingMode: WaitForFirstConsumer parameters: datastoreurl: ds:///vmfs/volumes/0e65b0b0-00cd0c66/ allowedTopologies: - matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - zone-b - key: failure-domain.beta.kubernetes.io/region values: - region-1","title":"WaitForFirstConsumer volume binding mode "},{"location":"volume-expansion/","text":"HPE SimpliVity CSI Driver - Offline Volume Expansion CSI Volume Expansion was introduced as an alpha feature in Kubernetes 1.14 and it was promoted to beta in Kubernetes 1.16. CSI Offline Volume Expansion is available from v2.0.0 of the HPE SimpliVity Driver. Currently, online expansion(ie, When the PVC is being used by a Pod i.e it is mounted on a node, the resulting volume expansion) is not supported. The offline volume expansion feature of CSI is basically the ability to extend / grow a Kubernetes Persistent Volume (PV) when it is not attached to a node. Currently, Offline Volume Expansion is supported only on dynamically created volumes. Requirements Make sure all the ESXi hosts of the cluster are on the same version as the vCenter. For volume expansion to work, the vCenter and all the ESX hosts of the cluster need to be on the supported versions of the feature i.e version 7.0 and above, otherwise volume resize operation fails with A general system error occurred: Failed to lock the file: api = DiskLib_Grow error. Offline Volume Expansion Create a new StorageClass or edit the existing StorageClass to set allowVolumeExpansion to true. # Contents of example-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com allowVolumeExpansion: true reclaimPolicy: Delete parameters: # datastoreurl and storagepolicyname are mutually exclusive. # datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter # storagepolicyname: \"policy-name\" # Policy on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" Create this StorageClass into the Kubernetes Cluster: $ kubectl create -f example-sc.yaml Define a PersistentVolumeClaim using the above StorageClass and create a PersistentVolumeClaim in the Kubernetes Cluster: # Contents of example-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: example-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: simplivity-sc $ kubectl create -f example-pvc.yaml Now patch the created PVC to increase its requested storage size (in this case, to 4Gi): # Alternatively, the size can be changed by modifying the spec using kubectl edit pvc example-pvc $ kubectl patch pvc example-pvc -p '{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"4Gi\"}}}}' persistentvolumeclaim/example-pvc patched This will trigger an expansion in the volume associated with the PVC in vSphere Cloud Native Storage and also gets reflected on the capacity of the corresponding PV $ kubectl describe pvc example-pvc Name: example-pvc Namespace: default StorageClass: simplivity-sc Status: Bound Volume: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 4Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 3m48s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 External provisioner is provisioning volume for claim \"default/example-pvc\" Normal ExternalProvisioning 3m8s (x4 over 3m48s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.simplivity.hpe.com\" or manually created by system administrator Normal ProvisioningSucceeded 2m56s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 Successfully provisioned volume pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da 4Gi RWO Delete Bound default/example-pvc simplivity-sc 8m27s $ kubectl describe pv pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Name: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: simplivity-sc Status: Bound Claim: default/example-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 4Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.simplivity.hpe.com VolumeHandle: d3cff17d-334a-4217-b55d-e478648c5b8d ReadOnly: false VolumeAttributes: datastoreurl=ds:///vmfs/volumes/9c8391e9-05250c25/ fstype=ext4 storage.kubernetes.io/csiProvisionerIdentity=1589547226925-8081-csi.simplivity.hpe.com type=HPE SimpliVity CNS Block Volume Events: <none> Note that if a PVC, that is being used by a pod, is expanded, then the expansion will fail. However, once the pod is no longer using the PVC, then the expansion that failed earlier, will be attempted.","title":"Offline Volume Expansion"},{"location":"volume-expansion/#hpe-simplivity-csi-driver-offline-volume-expansion","text":"CSI Volume Expansion was introduced as an alpha feature in Kubernetes 1.14 and it was promoted to beta in Kubernetes 1.16. CSI Offline Volume Expansion is available from v2.0.0 of the HPE SimpliVity Driver. Currently, online expansion(ie, When the PVC is being used by a Pod i.e it is mounted on a node, the resulting volume expansion) is not supported. The offline volume expansion feature of CSI is basically the ability to extend / grow a Kubernetes Persistent Volume (PV) when it is not attached to a node. Currently, Offline Volume Expansion is supported only on dynamically created volumes.","title":"HPE SimpliVity CSI Driver - Offline Volume Expansion"},{"location":"volume-expansion/#requirements","text":"Make sure all the ESXi hosts of the cluster are on the same version as the vCenter. For volume expansion to work, the vCenter and all the ESX hosts of the cluster need to be on the supported versions of the feature i.e version 7.0 and above, otherwise volume resize operation fails with A general system error occurred: Failed to lock the file: api = DiskLib_Grow error.","title":"Requirements"},{"location":"volume-expansion/#offline-volume-expansion","text":"Create a new StorageClass or edit the existing StorageClass to set allowVolumeExpansion to true. # Contents of example-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: simplivity-sc annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi.simplivity.hpe.com allowVolumeExpansion: true reclaimPolicy: Delete parameters: # datastoreurl and storagepolicyname are mutually exclusive. # datastoreurl: \"ds:///vmfs/volumes/9c8391e9-05250c25/\" # Storage URL, found under storage tab in vCenter # storagepolicyname: \"policy-name\" # Policy on selected datastore, from vCenter # Optional Parameter fstype: \"ext4\" Create this StorageClass into the Kubernetes Cluster: $ kubectl create -f example-sc.yaml Define a PersistentVolumeClaim using the above StorageClass and create a PersistentVolumeClaim in the Kubernetes Cluster: # Contents of example-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: example-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: simplivity-sc $ kubectl create -f example-pvc.yaml Now patch the created PVC to increase its requested storage size (in this case, to 4Gi): # Alternatively, the size can be changed by modifying the spec using kubectl edit pvc example-pvc $ kubectl patch pvc example-pvc -p '{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"4Gi\"}}}}' persistentvolumeclaim/example-pvc patched This will trigger an expansion in the volume associated with the PVC in vSphere Cloud Native Storage and also gets reflected on the capacity of the corresponding PV $ kubectl describe pvc example-pvc Name: example-pvc Namespace: default StorageClass: simplivity-sc Status: Bound Volume: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 4Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 3m48s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 External provisioner is provisioning volume for claim \"default/example-pvc\" Normal ExternalProvisioning 3m8s (x4 over 3m48s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.simplivity.hpe.com\" or manually created by system administrator Normal ProvisioningSucceeded 2m56s csi.simplivity.hpe.com_svt-csi-controller-0_e9cdfeaf-603c-45a4-837b-3e5b86beb6d7 Successfully provisioned volume pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da 4Gi RWO Delete Bound default/example-pvc simplivity-sc 8m27s $ kubectl describe pv pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Name: pvc-f6fb41c4-fda5-4768-9a68-3cb8b27967da Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.simplivity.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: simplivity-sc Status: Bound Claim: default/example-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 4Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.simplivity.hpe.com VolumeHandle: d3cff17d-334a-4217-b55d-e478648c5b8d ReadOnly: false VolumeAttributes: datastoreurl=ds:///vmfs/volumes/9c8391e9-05250c25/ fstype=ext4 storage.kubernetes.io/csiProvisionerIdentity=1589547226925-8081-csi.simplivity.hpe.com type=HPE SimpliVity CNS Block Volume Events: <none> Note that if a PVC, that is being used by a pod, is expanded, then the expansion will fail. However, once the pod is no longer using the PVC, then the expansion that failed earlier, will be attempted.","title":"Offline Volume Expansion"},{"location":"driver-deployment/configuring-topologies/","text":"Deployment with Topology for HPE SimpliVity CSI Driver for vSphere In order to enable topology aware provisioning, you need to perform the following steps when deploying the Kubernetes cluster: Set Up Zones in an HPE SimpliVity Environment Enable Zones for the vSphere CPI and HPE SimpliVity CSI Driver See Using Topologies for more information on when you might want to enable this feature and how to use it. Set Up Zones in an HPE SimpliVity Environment The zone configuration will depend on your HPE SimpliVity environment. A common scenario is to create a zone per HPE SimpliVity cluster as the HPE SimpliVity datastores are available to all HPE OmniStack hosts in the cluster. In the following example, the HPE SimpliVity environment includes three clusters with Kubernetes node VMs located on all three clusters. Create Zones Using vSphere Tags Use vSphere tags to label zones in your vSphere environment. In this example, the HPE SimpliVity environment includes three clusters: cluster1, cluster2, and cluster3. The Kubernetes node VMs are distributed across the three clusters. Category tags are created for the Kubernetes regions and zones. Then the datacenter is tagged with a region and the clusters are tagged with zones. Make sure that you have appropriate tagging privileges that control your ability to work with tags. See vSphere Tagging Privileges in the vSphere Security documentation. Note: Ancestors of node VMs, such as host, cluster, and data center, must have the ReadOnly role set for the vSphere user configured to use the CSI driver and CCM. This is required to allow reading tags and categories to prepare nodes' topology. In the vSphere Client create two tag categories that will be used for identifying regions and zones in your environment. In this example, the categories are named k8s-region and k8s-zone. For information, see Create, Edit, or Delete a Tag Category in the vCenter Server and Host Management documentation. Create tags for each zone and region in your environment. This example has one region and three zones (one per cluster). Categories Tags k8s-region region-1 k8s-zone zone-a, zone-b, zone-c Apply corresponding tags to the data center and clusters as indicated in the table. For information, see Assign or Remove a Tag in the vCenter Server and Host Management documentation. Categories Tags datacenter region-1 cluster1 zone-a cluster1 zone-b cluster1 zone-c Enable Zones for the vSphere CPI and HPE SimpliVity CSI Driver for vSphere After creating and applying topology tags in vSphere, the next step is to install the vSphere CPI and the HPE SimpliVity CSI driver using the zone and region categories. Add Labels section to cloud-config when installing the vSphere CPI. Add a Labels section to the configmap cloud-config file, specifying the category names you created for the region and zone. [Global] insecure-flag = \"true\" # Set to true to use self-signed certificate [VirtualCenter \"1.1.1.1\"] # vCenter IP insecure-flag = \"true\" # Set to true to use self-signed certificate user = \"administrator\" # Login ID password = \"password\" # Login Password port = \"443\" # vCenter Server Port, Default Port 443 datacenters = \"datacenter\" # Comma separated list of Datacenter names where Kubernetes node VMs are present. [Labels] region = k8s-region # vSphere category name for Kubernetes region zone = k8s-zone # vSphere category name for Kubernetes zone Verify that your zones and regions are applied to the Kubernetes nodes. After installation, labels failure-domain.beta.kubernetes.io/region and failure-domain.beta.kubernetes.io/zone are applied to all nodes. $ kubectl get nodes -L failure-domain.beta.kubernetes.io/zone -L failure-domain.beta.kubernetes.io/region NAME STATUS ROLES AGE VERSION ZONE REGION k8s-r1za-m01 Ready master 13h v1.17.5 zone-a region-1 k8s-r1za-w01 Ready <none> 13h v1.17.5 zone-a region-1 k8s-r1za-w02 Ready <none> 13h v1.17.5 zone-a region-1 k8s-r1za-w03 Ready <none> 13h v1.17.5 zone-a region-1 k8s-r1zb-m01 Ready master 13h v1.17.5 zone-b region-1 k8s-r1zb-w01 Ready <none> 13h v1.17.5 zone-b region-1 k8s-r1zb-w02 Ready <none> 13h v1.17.5 zone-b region-1 k8s-r1zb-w03 Ready <none> 13h v1.17.5 zone-b region-1 k8s-r1zc-m01 Ready master 13h v1.17.5 zone-c region-1 k8s-r1zc-w01 Ready <none> 13h v1.17.5 zone-c region-1 k8s-r1zc-w02 Ready <none> 13h v1.17.5 zone-c region-1 k8s-r1zc-w03 Ready <none> 13h v1.17.5 zone-c region-1 Add Labels section to cloud-config when installing the CSI driver. In the credential secret file, add a Labels section with entries for the region and zone categories you created in vSphere. [Global] cluster-id = \"demo-cluster-id\" # Cluster Name, Each Kubernetes cluster should have it's own unique cluster-id set in the csi-vsphere.conf file [VirtualCenter \"1.1.1.1\"] # vCenter IP insecure-flag = \"true\" # Set to true to use self-signed certificate user = \"administrator\" # Login ID password = \"password\" # Login Password port = \"443\" # vCenter Server Port, Default Port 443 datacenters = \"datacenter\" # Comma separated list of Datacenter names where Kubernetes node VMs are present. [HPESimpliVity] ip = \"1.1.1.2\" # Management Virtual Appliance (MVA) IP Address or HPE SimpliVity OVC Management IP address user = \"simplivity_user\" # Login ID of HPE SimpliVity OVC password = \"simplivity_password\" # Password of HPE SimpliVity OVC [Labels] region = k8s-region # vSphere category name for Kubernetes region zone = k8s-zone # vSphere category name for Kubernetes zone Make sure external-provisioner is deployed with the arguments --feature-gates=Topology=true and --strict-topology enabled. Uncomment the lines following the comment 'needed for topology aware setup' in the svt-csi-controller-deployment.yaml file. Verify that your CSI driver installation has the topology feature enabled. $ kubectl get csinodes -o jsonpath='{range .items[*]}{.metadata.name} {.spec}{\"\\n\"}{end}' k8s-r1za-m01 map[drivers:<nil>] k8s-r1za-w01 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1za-w01 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1za-w02 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1za-w02 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1za-w03 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1za-w03 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zb-m01 map[drivers:<nil>] k8s-r1zb-w01 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zb-w01 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zb-w02 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zb-w02 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zb-w03 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zb-w03 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zc-m01 map[drivers:<nil>] k8s-r1zc-w01 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zc-w01 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zc-w02 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zc-w02 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zc-w03 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zc-w03 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]]","title":"Configuring Topologies"},{"location":"driver-deployment/configuring-topologies/#deployment-with-topology-for-hpe-simplivity-csi-driver-for-vsphere","text":"In order to enable topology aware provisioning, you need to perform the following steps when deploying the Kubernetes cluster: Set Up Zones in an HPE SimpliVity Environment Enable Zones for the vSphere CPI and HPE SimpliVity CSI Driver See Using Topologies for more information on when you might want to enable this feature and how to use it.","title":"Deployment with Topology for HPE SimpliVity CSI Driver for vSphere"},{"location":"driver-deployment/configuring-topologies/#set-up-zones-in-an-hpe-simplivity-environment","text":"The zone configuration will depend on your HPE SimpliVity environment. A common scenario is to create a zone per HPE SimpliVity cluster as the HPE SimpliVity datastores are available to all HPE OmniStack hosts in the cluster. In the following example, the HPE SimpliVity environment includes three clusters with Kubernetes node VMs located on all three clusters.","title":"Set Up Zones in an HPE SimpliVity Environment "},{"location":"driver-deployment/configuring-topologies/#create-zones-using-vsphere-tags","text":"Use vSphere tags to label zones in your vSphere environment. In this example, the HPE SimpliVity environment includes three clusters: cluster1, cluster2, and cluster3. The Kubernetes node VMs are distributed across the three clusters. Category tags are created for the Kubernetes regions and zones. Then the datacenter is tagged with a region and the clusters are tagged with zones. Make sure that you have appropriate tagging privileges that control your ability to work with tags. See vSphere Tagging Privileges in the vSphere Security documentation. Note: Ancestors of node VMs, such as host, cluster, and data center, must have the ReadOnly role set for the vSphere user configured to use the CSI driver and CCM. This is required to allow reading tags and categories to prepare nodes' topology. In the vSphere Client create two tag categories that will be used for identifying regions and zones in your environment. In this example, the categories are named k8s-region and k8s-zone. For information, see Create, Edit, or Delete a Tag Category in the vCenter Server and Host Management documentation. Create tags for each zone and region in your environment. This example has one region and three zones (one per cluster). Categories Tags k8s-region region-1 k8s-zone zone-a, zone-b, zone-c Apply corresponding tags to the data center and clusters as indicated in the table. For information, see Assign or Remove a Tag in the vCenter Server and Host Management documentation. Categories Tags datacenter region-1 cluster1 zone-a cluster1 zone-b cluster1 zone-c","title":"Create Zones Using vSphere Tags"},{"location":"driver-deployment/configuring-topologies/#enable-zones-for-the-vsphere-cpi-and-hpe-simplivity-csi-driver-for-vsphere","text":"After creating and applying topology tags in vSphere, the next step is to install the vSphere CPI and the HPE SimpliVity CSI driver using the zone and region categories. Add Labels section to cloud-config when installing the vSphere CPI. Add a Labels section to the configmap cloud-config file, specifying the category names you created for the region and zone. [Global] insecure-flag = \"true\" # Set to true to use self-signed certificate [VirtualCenter \"1.1.1.1\"] # vCenter IP insecure-flag = \"true\" # Set to true to use self-signed certificate user = \"administrator\" # Login ID password = \"password\" # Login Password port = \"443\" # vCenter Server Port, Default Port 443 datacenters = \"datacenter\" # Comma separated list of Datacenter names where Kubernetes node VMs are present. [Labels] region = k8s-region # vSphere category name for Kubernetes region zone = k8s-zone # vSphere category name for Kubernetes zone Verify that your zones and regions are applied to the Kubernetes nodes. After installation, labels failure-domain.beta.kubernetes.io/region and failure-domain.beta.kubernetes.io/zone are applied to all nodes. $ kubectl get nodes -L failure-domain.beta.kubernetes.io/zone -L failure-domain.beta.kubernetes.io/region NAME STATUS ROLES AGE VERSION ZONE REGION k8s-r1za-m01 Ready master 13h v1.17.5 zone-a region-1 k8s-r1za-w01 Ready <none> 13h v1.17.5 zone-a region-1 k8s-r1za-w02 Ready <none> 13h v1.17.5 zone-a region-1 k8s-r1za-w03 Ready <none> 13h v1.17.5 zone-a region-1 k8s-r1zb-m01 Ready master 13h v1.17.5 zone-b region-1 k8s-r1zb-w01 Ready <none> 13h v1.17.5 zone-b region-1 k8s-r1zb-w02 Ready <none> 13h v1.17.5 zone-b region-1 k8s-r1zb-w03 Ready <none> 13h v1.17.5 zone-b region-1 k8s-r1zc-m01 Ready master 13h v1.17.5 zone-c region-1 k8s-r1zc-w01 Ready <none> 13h v1.17.5 zone-c region-1 k8s-r1zc-w02 Ready <none> 13h v1.17.5 zone-c region-1 k8s-r1zc-w03 Ready <none> 13h v1.17.5 zone-c region-1 Add Labels section to cloud-config when installing the CSI driver. In the credential secret file, add a Labels section with entries for the region and zone categories you created in vSphere. [Global] cluster-id = \"demo-cluster-id\" # Cluster Name, Each Kubernetes cluster should have it's own unique cluster-id set in the csi-vsphere.conf file [VirtualCenter \"1.1.1.1\"] # vCenter IP insecure-flag = \"true\" # Set to true to use self-signed certificate user = \"administrator\" # Login ID password = \"password\" # Login Password port = \"443\" # vCenter Server Port, Default Port 443 datacenters = \"datacenter\" # Comma separated list of Datacenter names where Kubernetes node VMs are present. [HPESimpliVity] ip = \"1.1.1.2\" # Management Virtual Appliance (MVA) IP Address or HPE SimpliVity OVC Management IP address user = \"simplivity_user\" # Login ID of HPE SimpliVity OVC password = \"simplivity_password\" # Password of HPE SimpliVity OVC [Labels] region = k8s-region # vSphere category name for Kubernetes region zone = k8s-zone # vSphere category name for Kubernetes zone Make sure external-provisioner is deployed with the arguments --feature-gates=Topology=true and --strict-topology enabled. Uncomment the lines following the comment 'needed for topology aware setup' in the svt-csi-controller-deployment.yaml file. Verify that your CSI driver installation has the topology feature enabled. $ kubectl get csinodes -o jsonpath='{range .items[*]}{.metadata.name} {.spec}{\"\\n\"}{end}' k8s-r1za-m01 map[drivers:<nil>] k8s-r1za-w01 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1za-w01 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1za-w02 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1za-w02 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1za-w03 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1za-w03 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zb-m01 map[drivers:<nil>] k8s-r1zb-w01 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zb-w01 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zb-w02 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zb-w02 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zb-w03 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zb-w03 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zc-m01 map[drivers:<nil>] k8s-r1zc-w01 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zc-w01 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zc-w02 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zc-w02 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]] k8s-r1zc-w03 map[drivers:[map[name:csi.simplivity.hpe.com nodeID:k8s-r1zc-w03 topologyKeys:[failure-domain.beta.kubernetes.io/region failure-domain.beta.kubernetes.io/zone]]]]","title":"Enable Zones for the vSphere CPI and HPE SimpliVity CSI Driver for vSphere "},{"location":"driver-deployment/installation/","text":"Install HPE SimpliVity Container Storage Interface (CSI) driver for vSphere After the VMware CPI and CSI snapshot controller are installed successfully, install the HPE SimpliVity CSI Driver. If those have not been installed please visit the prerequisites page before proceeding. Note that this installation guide only applies to Vanilla Kubernetes clusters on VMware All steps are performed from the master node Create a CSI Secret For the driver to access both VMware and the HPE SimpliVity Appliance, a set of credentials need to be passed in through a Kubernetes secret. This is also where the topology can be defined. Create a file csi-vsphere.conf [Global] cluster-id = \"demo-cluster-id\" # Cluster Name, Each Kubernetes cluster should have it's own unique cluster-id set in the csi-vsphere.conf file [VirtualCenter \"1.1.1.1\"] # vCenter IP insecure-flag = \"true\" # Set to true to use self-signed certificate user = \"administrator\" # Login ID password = \"password\" # Login Password port = \"443\" # vCenter Server Port, Default Port 443 datacenters = \"datacenter\" # Comma separated list of Datacenter names where Kubernetes node VMs are present. [HPESimpliVity] ip = \"1.1.1.2\" # HPE SimpliVity Management Virtual Appliance (MVA) IP Address or Management IP address user = \"simplivity_user\" # Login ID of HPE SimpliVity OVC password = \"simplivity_password\" # Password of HPE SimpliVity OVC ### Enable regions and zones to the topology # [Labels] # region = k8s-region # zone = k8s-zone Create the secret $ kubectl create secret generic vsphere-config-secret --from-file=csi-vsphere.conf --namespace=kube-system $ kubectl get secret vsphere-config-secret --namespace=kube-system NAME TYPE DATA AGE vsphere-config-secret Opaque 1 12s After creating the secret in Kubernetes, remove the file for security. rm csi-vsphere.conf Create RBAC for HPE SimpliVity CSI Driver Download and install the appropriate RBAC yaml from the HPE Simplivity CSI Driver GitHub repo . This creates a Role, ServiceAccount and ClusterRoleBindings for the HPE SimpliVity Driver. $ kubectl apply -f svt-csi-controller-rbac.yaml serviceaccount/svt-csi-controller created clusterrole.rbac.authorization.k8s.io/svt-csi-controller-role created clusterrolebinding.rbac.authorization.k8s.io/svt-csi-controller-binding created Install HPE SimpliVity CSI Driver Download and install the appropriate HPE SimpliVity controller-deployment and node-ds yaml files from the HPE Simplivity CSI Driver GitHub repo . The controller-deployment has the Deployment for the CSI controller, CSI attacher, CSI Provisioner and SVT syncer pods (the latter is used by our new Cloud Native Storage feature). The node-ds is a DaemonSet for the CSI component that will run on every worker node. It also has the definition for some of the new CRDs (Custom Resource Definitions) which we shall see shortly. Once again use kubectl to import the manifest into your cluster. $ kubectl apply -f svt-csi-controller-deployment.yaml deployment.apps/svt-csi-controller created csidriver.storage.k8s.io/csi.simplivity.hpe.com created $ kubectl apply -f svt-csi-node-ds.yaml daemonset.apps/svt-csi-node created Verify that the CSI Driver successfully installed Verify CSI Controller $ kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE coredns 2/2 2 2 30d svt-csi-controller 1/1 1 1 37m Check that the CSI is running. There should be one CSI node per worker node in the cluster (this example has 2 worker nodes) $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE coredns-7988585f4-gww47 1/1 Running 0 20m14s coredns-7988585f4-z9l7l 1/1 Running 0 20m14s etcd-2002210051-k8s-master-1 1/1 Running 0 20m9s kube-apiserver-2002210051-k8s-master-1 1/1 Running 0 20m9s kube-controller-manager-2002210051-k8s-master-1 1/1 Running 0 20m9s kube-flannel-ds-amd64-kbzbv 1/1 Running 0 16m44s kube-flannel-ds-amd64-mwpxm 1/1 Running 0 20m7s kube-flannel-ds-amd64-snxqz 1/1 Running 2 15m6s kube-proxy-4sv8n 1/1 Running 0 15m6s kube-proxy-ccltk 1/1 Running 0 16m44s kube-proxy-kdgm9 1/1 Running 0 20m15s kube-scheduler-2002210051-k8s-master-1 1/1 Running 0 20m9s snapshot-controller-0 1/1 Running 0 11m svt-csi-controller-0 6/6 Running 0 4m1s svt-csi-node-jxrfr 3/3 Running 0 3m5s svt-csi-node-rdj99 3/3 Running 0 4m1s vsphere-cloud-controller-manager-twcg5 1/1 Running 0 4m56s Verify that the CSI Custom Resource Definitions are working $ kubectl get CSINode NAME CREATED AT 2002210051-k8s-master-1 2020-02-21T01:04:54Z 2002210051-k8s-worker-1 2020-02-21T01:06:43Z 2002210051-k8s-worker-2 2020-02-21T01:08:22Z $ kubectl describe CSINode Name: 2002210051-k8s-master-1 Namespace: Labels: <none> Annotations: <none> API Version: storage.k8s.io/v1 Kind: CSINode Metadata: Creation Timestamp: 2020-02-21T01:04:54Z Owner References: API Version: v1 Kind: Node Name: 2002210051-k8s-master-1 UID: 20efb15c-3925-4ace-8800-dc3a21b26ad5 Resource Version: 40 Self Link: /apis/storage.k8s.io/v1/csinodes/2002210051-k8s-master-1 UID: 09241566-19b8-4332-9536-03ef984e671b Spec: Drivers: <nil> Events: <none> Name: 2002210051-k8s-worker-1 Namespace: Labels: <none> Annotations: <none> API Version: storage.k8s.io/v1 Kind: CSINode Metadata: Creation Timestamp: 2020-02-21T01:06:43Z Owner References: API Version: v1 Kind: Node Name: 2002210051-k8s-worker-1 UID: aef35e9d-2291-4f52-a5b6-7514e47d6781 Resource Version: 1346 Self Link: /apis/storage.k8s.io/v1/csinodes/2002210051-k8s-worker-1 UID: a4d5abcf-51d9-438e-8567-d7f7ace93bfd Spec: Drivers: Name: csi.simplivity.hpe.com Node ID: 2002210051-k8s-worker-1 Topology Keys: <nil> Events: <none> Name: 2002210051-k8s-worker-2 Namespace: Labels: <none> Annotations: <none> API Version: storage.k8s.io/v1 Kind: CSINode Metadata: Creation Timestamp: 2020-02-21T01:08:22Z Owner References: API Version: v1 Kind: Node Name: 2002210051-k8s-worker-2 UID: 089e54ff-7078-450b-a37f-ac57b928bd32 Resource Version: 1822 Self Link: /apis/storage.k8s.io/v1/csinodes/2002210051-k8s-worker-2 UID: 194ca6e4-e915-46bf-840f-f7f32badf361 Spec: Drivers: Name: csi.simplivity.hpe.com Node ID: 2002210051-k8s-worker-2 Topology Keys: <nil> Events: <none> Verify the CSI Driver $ kubectl get csidrivers NAME CREATED AT csi.simplivity.hpe.com 2020-02-21T01:09:26Z $ kubectl describe csidrivers Name: csi.simplivity.hpe.com Namespace: Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"storage.k8s.io/v1beta1\",\"kind\":\"CSIDriver\",\"metadata\":{\"annotations\":{},\"name\":\"csi.simplivity.hpe.com\"},\"spec\":{\"attachReq... API Version: storage.k8s.io/v1beta1 Kind: CSIDriver Metadata: Creation Timestamp: 2020-06-12T15:41:23Z Resource Version: 7766961 Self Link: /apis/storage.k8s.io/v1beta1/csidrivers/csi.simplivity.hpe.com UID: 32306d10-cbaf-4091-b234-89d948e25a5a Spec: Attach Required: true Pod Info On Mount: false Volume Lifecycle Modes: Persistent Events: <none> Verify cluster setup and get the provider IDs for each Node $ kubectl get nodes NAME STATUS ROLES AGE VERSION 2002210051-k8s-master-1 Ready master 13m v1.17.0 2002210051-k8s-worker-1 Ready <none> 11m v1.17.0 2002210051-k8s-worker-2 Ready <none> 9m38s v1.17.0 $ kubectl describe nodes | grep \"ProviderID\" ProviderID: vsphere://4238f1df-c5ba-30da-3ad6-18431ebe5f6a ProviderID: vsphere://4238b026-11d2-886b-74aa-5e2ec47a5657 ProviderID: vsphere://42383955-c3c1-26eb-2f60-66331c393847","title":"Installation"},{"location":"driver-deployment/installation/#install-hpe-simplivity-container-storage-interface-csi-driver-for-vsphere","text":"After the VMware CPI and CSI snapshot controller are installed successfully, install the HPE SimpliVity CSI Driver. If those have not been installed please visit the prerequisites page before proceeding. Note that this installation guide only applies to Vanilla Kubernetes clusters on VMware All steps are performed from the master node","title":"Install HPE SimpliVity Container Storage Interface (CSI) driver for vSphere"},{"location":"driver-deployment/installation/#create-a-csi-secret","text":"For the driver to access both VMware and the HPE SimpliVity Appliance, a set of credentials need to be passed in through a Kubernetes secret. This is also where the topology can be defined. Create a file csi-vsphere.conf [Global] cluster-id = \"demo-cluster-id\" # Cluster Name, Each Kubernetes cluster should have it's own unique cluster-id set in the csi-vsphere.conf file [VirtualCenter \"1.1.1.1\"] # vCenter IP insecure-flag = \"true\" # Set to true to use self-signed certificate user = \"administrator\" # Login ID password = \"password\" # Login Password port = \"443\" # vCenter Server Port, Default Port 443 datacenters = \"datacenter\" # Comma separated list of Datacenter names where Kubernetes node VMs are present. [HPESimpliVity] ip = \"1.1.1.2\" # HPE SimpliVity Management Virtual Appliance (MVA) IP Address or Management IP address user = \"simplivity_user\" # Login ID of HPE SimpliVity OVC password = \"simplivity_password\" # Password of HPE SimpliVity OVC ### Enable regions and zones to the topology # [Labels] # region = k8s-region # zone = k8s-zone Create the secret $ kubectl create secret generic vsphere-config-secret --from-file=csi-vsphere.conf --namespace=kube-system $ kubectl get secret vsphere-config-secret --namespace=kube-system NAME TYPE DATA AGE vsphere-config-secret Opaque 1 12s After creating the secret in Kubernetes, remove the file for security. rm csi-vsphere.conf","title":"Create a CSI Secret"},{"location":"driver-deployment/installation/#create-rbac-for-hpe-simplivity-csi-driver","text":"Download and install the appropriate RBAC yaml from the HPE Simplivity CSI Driver GitHub repo . This creates a Role, ServiceAccount and ClusterRoleBindings for the HPE SimpliVity Driver. $ kubectl apply -f svt-csi-controller-rbac.yaml serviceaccount/svt-csi-controller created clusterrole.rbac.authorization.k8s.io/svt-csi-controller-role created clusterrolebinding.rbac.authorization.k8s.io/svt-csi-controller-binding created","title":"Create RBAC for HPE SimpliVity CSI Driver"},{"location":"driver-deployment/installation/#install-hpe-simplivity-csi-driver","text":"Download and install the appropriate HPE SimpliVity controller-deployment and node-ds yaml files from the HPE Simplivity CSI Driver GitHub repo . The controller-deployment has the Deployment for the CSI controller, CSI attacher, CSI Provisioner and SVT syncer pods (the latter is used by our new Cloud Native Storage feature). The node-ds is a DaemonSet for the CSI component that will run on every worker node. It also has the definition for some of the new CRDs (Custom Resource Definitions) which we shall see shortly. Once again use kubectl to import the manifest into your cluster. $ kubectl apply -f svt-csi-controller-deployment.yaml deployment.apps/svt-csi-controller created csidriver.storage.k8s.io/csi.simplivity.hpe.com created $ kubectl apply -f svt-csi-node-ds.yaml daemonset.apps/svt-csi-node created","title":"Install HPE SimpliVity CSI Driver"},{"location":"driver-deployment/installation/#verify-that-the-csi-driver-successfully-installed","text":"Verify CSI Controller $ kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE coredns 2/2 2 2 30d svt-csi-controller 1/1 1 1 37m Check that the CSI is running. There should be one CSI node per worker node in the cluster (this example has 2 worker nodes) $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE coredns-7988585f4-gww47 1/1 Running 0 20m14s coredns-7988585f4-z9l7l 1/1 Running 0 20m14s etcd-2002210051-k8s-master-1 1/1 Running 0 20m9s kube-apiserver-2002210051-k8s-master-1 1/1 Running 0 20m9s kube-controller-manager-2002210051-k8s-master-1 1/1 Running 0 20m9s kube-flannel-ds-amd64-kbzbv 1/1 Running 0 16m44s kube-flannel-ds-amd64-mwpxm 1/1 Running 0 20m7s kube-flannel-ds-amd64-snxqz 1/1 Running 2 15m6s kube-proxy-4sv8n 1/1 Running 0 15m6s kube-proxy-ccltk 1/1 Running 0 16m44s kube-proxy-kdgm9 1/1 Running 0 20m15s kube-scheduler-2002210051-k8s-master-1 1/1 Running 0 20m9s snapshot-controller-0 1/1 Running 0 11m svt-csi-controller-0 6/6 Running 0 4m1s svt-csi-node-jxrfr 3/3 Running 0 3m5s svt-csi-node-rdj99 3/3 Running 0 4m1s vsphere-cloud-controller-manager-twcg5 1/1 Running 0 4m56s Verify that the CSI Custom Resource Definitions are working $ kubectl get CSINode NAME CREATED AT 2002210051-k8s-master-1 2020-02-21T01:04:54Z 2002210051-k8s-worker-1 2020-02-21T01:06:43Z 2002210051-k8s-worker-2 2020-02-21T01:08:22Z $ kubectl describe CSINode Name: 2002210051-k8s-master-1 Namespace: Labels: <none> Annotations: <none> API Version: storage.k8s.io/v1 Kind: CSINode Metadata: Creation Timestamp: 2020-02-21T01:04:54Z Owner References: API Version: v1 Kind: Node Name: 2002210051-k8s-master-1 UID: 20efb15c-3925-4ace-8800-dc3a21b26ad5 Resource Version: 40 Self Link: /apis/storage.k8s.io/v1/csinodes/2002210051-k8s-master-1 UID: 09241566-19b8-4332-9536-03ef984e671b Spec: Drivers: <nil> Events: <none> Name: 2002210051-k8s-worker-1 Namespace: Labels: <none> Annotations: <none> API Version: storage.k8s.io/v1 Kind: CSINode Metadata: Creation Timestamp: 2020-02-21T01:06:43Z Owner References: API Version: v1 Kind: Node Name: 2002210051-k8s-worker-1 UID: aef35e9d-2291-4f52-a5b6-7514e47d6781 Resource Version: 1346 Self Link: /apis/storage.k8s.io/v1/csinodes/2002210051-k8s-worker-1 UID: a4d5abcf-51d9-438e-8567-d7f7ace93bfd Spec: Drivers: Name: csi.simplivity.hpe.com Node ID: 2002210051-k8s-worker-1 Topology Keys: <nil> Events: <none> Name: 2002210051-k8s-worker-2 Namespace: Labels: <none> Annotations: <none> API Version: storage.k8s.io/v1 Kind: CSINode Metadata: Creation Timestamp: 2020-02-21T01:08:22Z Owner References: API Version: v1 Kind: Node Name: 2002210051-k8s-worker-2 UID: 089e54ff-7078-450b-a37f-ac57b928bd32 Resource Version: 1822 Self Link: /apis/storage.k8s.io/v1/csinodes/2002210051-k8s-worker-2 UID: 194ca6e4-e915-46bf-840f-f7f32badf361 Spec: Drivers: Name: csi.simplivity.hpe.com Node ID: 2002210051-k8s-worker-2 Topology Keys: <nil> Events: <none> Verify the CSI Driver $ kubectl get csidrivers NAME CREATED AT csi.simplivity.hpe.com 2020-02-21T01:09:26Z $ kubectl describe csidrivers Name: csi.simplivity.hpe.com Namespace: Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"storage.k8s.io/v1beta1\",\"kind\":\"CSIDriver\",\"metadata\":{\"annotations\":{},\"name\":\"csi.simplivity.hpe.com\"},\"spec\":{\"attachReq... API Version: storage.k8s.io/v1beta1 Kind: CSIDriver Metadata: Creation Timestamp: 2020-06-12T15:41:23Z Resource Version: 7766961 Self Link: /apis/storage.k8s.io/v1beta1/csidrivers/csi.simplivity.hpe.com UID: 32306d10-cbaf-4091-b234-89d948e25a5a Spec: Attach Required: true Pod Info On Mount: false Volume Lifecycle Modes: Persistent Events: <none> Verify cluster setup and get the provider IDs for each Node $ kubectl get nodes NAME STATUS ROLES AGE VERSION 2002210051-k8s-master-1 Ready master 13m v1.17.0 2002210051-k8s-worker-1 Ready <none> 11m v1.17.0 2002210051-k8s-worker-2 Ready <none> 9m38s v1.17.0 $ kubectl describe nodes | grep \"ProviderID\" ProviderID: vsphere://4238f1df-c5ba-30da-3ad6-18431ebe5f6a ProviderID: vsphere://4238b026-11d2-886b-74aa-5e2ec47a5657 ProviderID: vsphere://42383955-c3c1-26eb-2f60-66331c393847","title":"Verify that the CSI Driver successfully installed"},{"location":"driver-deployment/uninstallation/","text":"Uninstalling the HPE SimpliVity Container Storage Interface (CSI) driver for vSphere Remove the Daemon Set, Deployment and RBAC for the HPE SimpliVity CSI Driver using the yaml files dowloaded earlier. $ kubectl delete -f svt-csi-node-ds.yaml daemonset.apps \"svt-csi-node\" deleted $ kubectl delete -f svt-csi-controller-deployment.yaml deployment.apps \"svt-csi-controller\" deleted csidriver.storage.k8s.io \"csi.simplivity.hpe.com\" deleted $ kubectl delete -f svt-csi-controller-rbac.yaml serviceaccount \"svt-csi-controller\" deleted clusterrole.rbac.authorization.k8s.io \"svt-csi-controller-role\" deleted clusterrolebinding.rbac.authorization.k8s.io \"svt-csi-controller-binding\" deleted Verify that CSI-Controller and CSI-Node were removed $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE coredns-7988585f4-5v7kj 1/1 Running 0 32h coredns-7988585f4-mpm6x 1/1 Running 0 32h etcd-2005121533-k8s-master-1 1/1 Running 0 32h kube-apiserver-2005121533-k8s-master-1 1/1 Running 0 32h kube-controller-manager-2005121533-k8s-master-1 1/1 Running 0 32h kube-flannel-ds-amd64-7qvrt 1/1 Running 0 32h kube-flannel-ds-amd64-n59fc 1/1 Running 0 32h kube-flannel-ds-amd64-x9hlx 1/1 Running 0 32h kube-proxy-7pvb7 1/1 Running 0 32h kube-proxy-hvgvq 1/1 Running 0 32h kube-proxy-kr6jp 1/1 Running 0 32h kube-scheduler-2005121533-k8s-master-1 1/1 Running 0 32h snapshot-controller-0 1/1 Running 0 32h vsphere-cloud-controller-manager-twcg5 1/1 Running 0 32h $ kubectl get deployment --namespace=kube-system NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system coredns 2/2 2 2 31d $ kubectl get daemonsets svt-csi-node --namespace=kube-system Error from server (NotFound): daemonsets.apps \"svt-csi-node\" not found $ kubectl get csidrivers No resources found in default namespace.","title":"Uninstallation"},{"location":"driver-deployment/uninstallation/#uninstalling-the-hpe-simplivity-container-storage-interface-csi-driver-for-vsphere","text":"Remove the Daemon Set, Deployment and RBAC for the HPE SimpliVity CSI Driver using the yaml files dowloaded earlier. $ kubectl delete -f svt-csi-node-ds.yaml daemonset.apps \"svt-csi-node\" deleted $ kubectl delete -f svt-csi-controller-deployment.yaml deployment.apps \"svt-csi-controller\" deleted csidriver.storage.k8s.io \"csi.simplivity.hpe.com\" deleted $ kubectl delete -f svt-csi-controller-rbac.yaml serviceaccount \"svt-csi-controller\" deleted clusterrole.rbac.authorization.k8s.io \"svt-csi-controller-role\" deleted clusterrolebinding.rbac.authorization.k8s.io \"svt-csi-controller-binding\" deleted Verify that CSI-Controller and CSI-Node were removed $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE coredns-7988585f4-5v7kj 1/1 Running 0 32h coredns-7988585f4-mpm6x 1/1 Running 0 32h etcd-2005121533-k8s-master-1 1/1 Running 0 32h kube-apiserver-2005121533-k8s-master-1 1/1 Running 0 32h kube-controller-manager-2005121533-k8s-master-1 1/1 Running 0 32h kube-flannel-ds-amd64-7qvrt 1/1 Running 0 32h kube-flannel-ds-amd64-n59fc 1/1 Running 0 32h kube-flannel-ds-amd64-x9hlx 1/1 Running 0 32h kube-proxy-7pvb7 1/1 Running 0 32h kube-proxy-hvgvq 1/1 Running 0 32h kube-proxy-kr6jp 1/1 Running 0 32h kube-scheduler-2005121533-k8s-master-1 1/1 Running 0 32h snapshot-controller-0 1/1 Running 0 32h vsphere-cloud-controller-manager-twcg5 1/1 Running 0 32h $ kubectl get deployment --namespace=kube-system NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system coredns 2/2 2 2 31d $ kubectl get daemonsets svt-csi-node --namespace=kube-system Error from server (NotFound): daemonsets.apps \"svt-csi-node\" not found $ kubectl get csidrivers No resources found in default namespace.","title":"Uninstalling the HPE SimpliVity Container Storage Interface (CSI) driver for vSphere"},{"location":"driver-deployment/prerequisites-deployment/install-cpi/","text":"Installing VMware Cloud Provider Interface (CPI) Following the VMware CPI guide here Login to the Master Node and create a config map file /etc/kubernetes/vsphere.conf that is passed into CPI on initialization. This is also where the topology is defined. # Contents of /etc/kubernetes/vsphere.conf [Global] # user = \"administrator@vsphere.local.com\" # (Do Not Use) Login to vCenter # password = \"DoNotUse\" # (Do Not Use) Password to vCenter port = \"443\" # vCenter Server port (Default: 443) insecure-flag = \"true\" # Set to true to use self-signed certificate secret-name = \"cpi-global-secret\" # k8s secret name <- we will create this later secret-namespace = \"kube-system\" # k8s secret namespace <- we will create this later [VirtualCenter \"10.1.1.21\"] # IP of the vCenter to connect to datacenters = \"Datacenter\" # Comma separated list of datacenters where kubernetes node VMs are present ### Examples of adding additional vCenters # [VirtualCenter \"192.168.0.1\"] # datacenters = \"hr\" # [VirtualCenter \"10.0.0.1\"] # datacenters = \"engineering\" # secret-name = \"cpi-engineering-secret\" # secret-namespace = \"kube-system\" ### Enable regions and zones to the topology # [Labels] # region = k8s-region # zone = k8s-zone Create a Cloud-Config for Kubernetes $ cd /etc/kubernetes $ kubectl create configmap cloud-config --from-file=vsphere.conf --namespace=kube-system Check that the Cloud Config was created $ kubectl get configmap cloud-config --namespace=kube-system NAME DATA AGE cloud-config 1 2m19s Create a K8s Secret YAML apiVersion: v1 kind: Secret metadata: name: cpi-global-secret # same secret-name provided in vsphere.conf namespace: kube-system # same secret-namespace provided in vsphere.conf stringData: 10.0.0.1.username: \"administrator\" # UPDATE the IP to match the vCenter IP 10.0.0.1.password: \"password\" # UPDATE the IP to match the vCenter IP # 192.168.0.1.username: \"administrator@vsphere.local\" # to add another vCenter to the same secret # 192.168.0.1.password: \"password\" Create the Secret $ kubectl create -f cpi-global-secret.yaml Topologies: Setting up Regions and Zones Kubernetes allows you to place Pods and Persisent Volumes on specific parts of the underlying infrastructure, e.g. different DataCenters or different vCenters, using the concept of Zones and Regions. However, to use placement controls, the required configuration steps needs to be put in place at Kubernetes deployment time, and require additional settings in the vSphere.conf of the CPI. For more information on how to implement zones/regions support, there is a zones/regions tutorial on how to do it here . If you are not interested in K8s object placement, this section can be ignored, and you can proceed with the remaining CPI setup steps. Check that all the nodes have Taints $ kubectl describe nodes | egrep \"Taints:|Name:\" Name: k8s-master Taints: node-role.kubernetes.io/master:NoSchedule Name: k8s-worker Taints: node.kubernetes.io/not-ready:NoExecute Deploy cloud controller modules. Please refer to the support information to download the appropriate yamls according to the driver version. # deploy the appropriate version of CPI based on the driver version, refer to the example given below for v1.20.0 $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/v1.20.0/manifests/controller-manager/cloud-controller-manager-roles.yaml clusterrole.rbac.authorization.k8s.io/system:cloud-controller-manager created $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/v1.20.0/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml rolebinding.rbac.authorization.k8s.io/servicecatalog.k8s.io:apiserver-authentication-reader created clusterrolebinding.rbac.authorization.k8s.io/system:cloud-controller-manager created $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/v1.20.0/manifests/controller-manager/vsphere-cloud-controller-manager-ds.yaml serviceaccount/cloud-controller-manager created daemonset.apps/vsphere-cloud-controller-manager created service/vsphere-cloud-controller-manager created Verify that the Cloud Controllers were deployed $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE coredns-7988585f4-ts5b5 1/1 Running 1 4d7h coredns-7988585f4-wb2bk 1/1 Running 1 4d7h etcd-k8s-master 1/1 Running 2 4d7h kube-apiserver-k8s-master 1/1 Running 2 4d7h kube-controller-manager-k8s-master 1/1 Running 3 4d7h kube-flannel-ds-amd64-lfh7s 1/1 Running 2 7h3m kube-flannel-ds-amd64-r5pl6 1/1 Running 1 11h kube-proxy-b2qv4 1/1 Running 2 4d7h kube-proxy-w4bdj 1/1 Running 0 7h3m kube-scheduler-k8s-master 1/1 Running 2 4d7h vsphere-cloud-controller-manager-n8pn2 1/1 Running 1 4h54m Check the taints $ sudo kubectl describe nodes | egrep \"Taints:|Name:\" Name: k8s-master Taints: node-role.kubernetes.io/master:NoSchedule Name: k8s-worker Taints: <none> The node-role.kubernetes.io/master=:NoSchedule taint is required to be present on the master nodes to prevent scheduling of the node plugin pods for the csi node daemonset on the master nodes. Should you need to re-add the taint, you can use the following command: $ kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule With the master and worker nodes setup, enabling snapshots is the next piece to setup for the HPE SimpliVity CSI. This requires the installation of the CSI snapshot controller.","title":"Install CPI"},{"location":"driver-deployment/prerequisites-deployment/install-cpi/#installing-vmware-cloud-provider-interface-cpi","text":"Following the VMware CPI guide here Login to the Master Node and create a config map file /etc/kubernetes/vsphere.conf that is passed into CPI on initialization. This is also where the topology is defined. # Contents of /etc/kubernetes/vsphere.conf [Global] # user = \"administrator@vsphere.local.com\" # (Do Not Use) Login to vCenter # password = \"DoNotUse\" # (Do Not Use) Password to vCenter port = \"443\" # vCenter Server port (Default: 443) insecure-flag = \"true\" # Set to true to use self-signed certificate secret-name = \"cpi-global-secret\" # k8s secret name <- we will create this later secret-namespace = \"kube-system\" # k8s secret namespace <- we will create this later [VirtualCenter \"10.1.1.21\"] # IP of the vCenter to connect to datacenters = \"Datacenter\" # Comma separated list of datacenters where kubernetes node VMs are present ### Examples of adding additional vCenters # [VirtualCenter \"192.168.0.1\"] # datacenters = \"hr\" # [VirtualCenter \"10.0.0.1\"] # datacenters = \"engineering\" # secret-name = \"cpi-engineering-secret\" # secret-namespace = \"kube-system\" ### Enable regions and zones to the topology # [Labels] # region = k8s-region # zone = k8s-zone Create a Cloud-Config for Kubernetes $ cd /etc/kubernetes $ kubectl create configmap cloud-config --from-file=vsphere.conf --namespace=kube-system Check that the Cloud Config was created $ kubectl get configmap cloud-config --namespace=kube-system NAME DATA AGE cloud-config 1 2m19s Create a K8s Secret YAML apiVersion: v1 kind: Secret metadata: name: cpi-global-secret # same secret-name provided in vsphere.conf namespace: kube-system # same secret-namespace provided in vsphere.conf stringData: 10.0.0.1.username: \"administrator\" # UPDATE the IP to match the vCenter IP 10.0.0.1.password: \"password\" # UPDATE the IP to match the vCenter IP # 192.168.0.1.username: \"administrator@vsphere.local\" # to add another vCenter to the same secret # 192.168.0.1.password: \"password\" Create the Secret $ kubectl create -f cpi-global-secret.yaml","title":"Installing VMware Cloud Provider Interface (CPI)"},{"location":"driver-deployment/prerequisites-deployment/install-cpi/#topologies-setting-up-regions-and-zones","text":"Kubernetes allows you to place Pods and Persisent Volumes on specific parts of the underlying infrastructure, e.g. different DataCenters or different vCenters, using the concept of Zones and Regions. However, to use placement controls, the required configuration steps needs to be put in place at Kubernetes deployment time, and require additional settings in the vSphere.conf of the CPI. For more information on how to implement zones/regions support, there is a zones/regions tutorial on how to do it here . If you are not interested in K8s object placement, this section can be ignored, and you can proceed with the remaining CPI setup steps.","title":"Topologies: Setting up Regions and Zones"},{"location":"driver-deployment/prerequisites-deployment/install-cpi/#check-that-all-the-nodes-have-taints","text":"$ kubectl describe nodes | egrep \"Taints:|Name:\" Name: k8s-master Taints: node-role.kubernetes.io/master:NoSchedule Name: k8s-worker Taints: node.kubernetes.io/not-ready:NoExecute Deploy cloud controller modules. Please refer to the support information to download the appropriate yamls according to the driver version. # deploy the appropriate version of CPI based on the driver version, refer to the example given below for v1.20.0 $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/v1.20.0/manifests/controller-manager/cloud-controller-manager-roles.yaml clusterrole.rbac.authorization.k8s.io/system:cloud-controller-manager created $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/v1.20.0/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml rolebinding.rbac.authorization.k8s.io/servicecatalog.k8s.io:apiserver-authentication-reader created clusterrolebinding.rbac.authorization.k8s.io/system:cloud-controller-manager created $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/v1.20.0/manifests/controller-manager/vsphere-cloud-controller-manager-ds.yaml serviceaccount/cloud-controller-manager created daemonset.apps/vsphere-cloud-controller-manager created service/vsphere-cloud-controller-manager created Verify that the Cloud Controllers were deployed $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE coredns-7988585f4-ts5b5 1/1 Running 1 4d7h coredns-7988585f4-wb2bk 1/1 Running 1 4d7h etcd-k8s-master 1/1 Running 2 4d7h kube-apiserver-k8s-master 1/1 Running 2 4d7h kube-controller-manager-k8s-master 1/1 Running 3 4d7h kube-flannel-ds-amd64-lfh7s 1/1 Running 2 7h3m kube-flannel-ds-amd64-r5pl6 1/1 Running 1 11h kube-proxy-b2qv4 1/1 Running 2 4d7h kube-proxy-w4bdj 1/1 Running 0 7h3m kube-scheduler-k8s-master 1/1 Running 2 4d7h vsphere-cloud-controller-manager-n8pn2 1/1 Running 1 4h54m Check the taints $ sudo kubectl describe nodes | egrep \"Taints:|Name:\" Name: k8s-master Taints: node-role.kubernetes.io/master:NoSchedule Name: k8s-worker Taints: <none> The node-role.kubernetes.io/master=:NoSchedule taint is required to be present on the master nodes to prevent scheduling of the node plugin pods for the csi node daemonset on the master nodes. Should you need to re-add the taint, you can use the following command: $ kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule With the master and worker nodes setup, enabling snapshots is the next piece to setup for the HPE SimpliVity CSI. This requires the installation of the CSI snapshot controller.","title":"Check that all the nodes have Taints"},{"location":"driver-deployment/prerequisites-deployment/install-csi-snapshot-controller/","text":"Installing CSI Snapshot Controller To enable the HPE SimpliVity CSI to take snapshots install the CSI Snapshot Controller on each cluster Download the yaml files from the GitHub repo of CSI Snapshotter. Refer to the support information page to download the appropriate yamls according to the driver version. CSI Snapshot CRDs CSI Snapshot Controller In the below examples we are using namespace kube-system , which is a good place to store infrastructure related resources in Kubernetes. In setup-snapshot-controller.yaml , update the namespace and image version. The CSI snapshot controller image version should be a compatible version from the table in Support Information . 11 metadata: 12 name: snapshot-controller 13 namespace: kube-system # Use kube-system namespace 14 spec: 27 - name: snapshot-controller 28 image: quay.io/k8scsi/snapshot-controller:v4.0.0 Update the namespace values in rbac-snapshot-controller.yaml 9 kind: ServiceAccount 10 metadata: 11 name: snapshot-controller 12 namespace: kube-system # Use kube-system namespace 13 14 --- 47 kind: ClusterRoleBinding 48 apiVersion: rbac.authorization.k8s.io/v1 49 metadata: 50 name: snapshot-controller-role 51 subjects: 52 - kind: ServiceAccount 53 name: snapshot-controller 54 # replace with non-default namespace name 55 namespace: kube-system # Use kube-system namespace 56 roleRef: 62 kind: Role 63 apiVersion: rbac.authorization.k8s.io/v1 64 metadata: 65 namespace: kube-system # Use kube-system namespace 66 name: snapshot-controller-leaderelection 73 kind: RoleBinding 74 apiVersion: rbac.authorization.k8s.io/v1 75 metadata: 76 name: snapshot-controller-leaderelection 77 namespace: kube-system # Use kube-system namespace 78 subjects: 79 - kind: ServiceAccount 80 name: snapshot-controller 81 namespace: kube-system # Use kube-system namespace (Optional) Enabling Fault Tolerance Update \"replica\" and \"leader-election\" values in setup-snapshot-controller.yaml 14 spec: 15 serviceName: \"snapshot-controller\" 16 replicas: 2 # Update value to 2 17 selector: 29 args: 30 - \"--v=5\" 31 - \"--leader-election=true\" # Update value to True 32 imagePullPolicy: Always Install the custom resource definition (CRD) for the Snapshot side car $ kubectl create -f snapshot.storage.k8s.io_volumesnapshotclasses.yaml $ kubectl create -f snapshot.storage.k8s.io_volumesnapshotcontents.yaml $ kubectl create -f snapshot.storage.k8s.io_volumesnapshots.yaml Install the snapshot-controller $ kubectl create -f rbac-snapshot-controller.yaml $ kubectl create -f setup-snapshot-controller.yaml","title":"Install CSI Snapshot Controller"},{"location":"driver-deployment/prerequisites-deployment/install-csi-snapshot-controller/#installing-csi-snapshot-controller","text":"To enable the HPE SimpliVity CSI to take snapshots install the CSI Snapshot Controller on each cluster Download the yaml files from the GitHub repo of CSI Snapshotter. Refer to the support information page to download the appropriate yamls according to the driver version. CSI Snapshot CRDs CSI Snapshot Controller In the below examples we are using namespace kube-system , which is a good place to store infrastructure related resources in Kubernetes. In setup-snapshot-controller.yaml , update the namespace and image version. The CSI snapshot controller image version should be a compatible version from the table in Support Information . 11 metadata: 12 name: snapshot-controller 13 namespace: kube-system # Use kube-system namespace 14 spec: 27 - name: snapshot-controller 28 image: quay.io/k8scsi/snapshot-controller:v4.0.0 Update the namespace values in rbac-snapshot-controller.yaml 9 kind: ServiceAccount 10 metadata: 11 name: snapshot-controller 12 namespace: kube-system # Use kube-system namespace 13 14 --- 47 kind: ClusterRoleBinding 48 apiVersion: rbac.authorization.k8s.io/v1 49 metadata: 50 name: snapshot-controller-role 51 subjects: 52 - kind: ServiceAccount 53 name: snapshot-controller 54 # replace with non-default namespace name 55 namespace: kube-system # Use kube-system namespace 56 roleRef: 62 kind: Role 63 apiVersion: rbac.authorization.k8s.io/v1 64 metadata: 65 namespace: kube-system # Use kube-system namespace 66 name: snapshot-controller-leaderelection 73 kind: RoleBinding 74 apiVersion: rbac.authorization.k8s.io/v1 75 metadata: 76 name: snapshot-controller-leaderelection 77 namespace: kube-system # Use kube-system namespace 78 subjects: 79 - kind: ServiceAccount 80 name: snapshot-controller 81 namespace: kube-system # Use kube-system namespace","title":"Installing CSI Snapshot Controller"},{"location":"driver-deployment/prerequisites-deployment/install-csi-snapshot-controller/#optional-enabling-fault-tolerance","text":"Update \"replica\" and \"leader-election\" values in setup-snapshot-controller.yaml 14 spec: 15 serviceName: \"snapshot-controller\" 16 replicas: 2 # Update value to 2 17 selector: 29 args: 30 - \"--v=5\" 31 - \"--leader-election=true\" # Update value to True 32 imagePullPolicy: Always","title":"(Optional) Enabling Fault Tolerance"},{"location":"driver-deployment/prerequisites-deployment/install-csi-snapshot-controller/#install-the-custom-resource-definition-crd-for-the-snapshot-side-car","text":"$ kubectl create -f snapshot.storage.k8s.io_volumesnapshotclasses.yaml $ kubectl create -f snapshot.storage.k8s.io_volumesnapshotcontents.yaml $ kubectl create -f snapshot.storage.k8s.io_volumesnapshots.yaml","title":"Install the custom resource definition (CRD) for the Snapshot side car"},{"location":"driver-deployment/prerequisites-deployment/install-csi-snapshot-controller/#install-the-snapshot-controller","text":"$ kubectl create -f rbac-snapshot-controller.yaml $ kubectl create -f setup-snapshot-controller.yaml","title":"Install the snapshot-controller"},{"location":"driver-deployment/prerequisites-deployment/prerequisites/","text":"Prerequisites for Installing HPE SimpliVity CSI Driver The following software must be installed prior to installing the HPE SimpliVity Container Storage Interface (CSI) Driver for vSphere: HPE OmniStack for vSphere VMware vSphere Kubernetes vSphere CPI Driver CSI Snapshot Controller Consult Support Information for software version compatibility. Installing Prerequisites Have an OS Template with Kubernetes Installed Setup Master and Worker Nodes in VMware Install VMware Cloud Provider Interface (CPI) For snapshot support install Kubernetes CSI Snapshot Controller","title":"Prerequisites"},{"location":"driver-deployment/prerequisites-deployment/prerequisites/#prerequisites-for-installing-hpe-simplivity-csi-driver","text":"The following software must be installed prior to installing the HPE SimpliVity Container Storage Interface (CSI) Driver for vSphere: HPE OmniStack for vSphere VMware vSphere Kubernetes vSphere CPI Driver CSI Snapshot Controller Consult Support Information for software version compatibility.","title":"Prerequisites for Installing HPE SimpliVity CSI Driver"},{"location":"driver-deployment/prerequisites-deployment/prerequisites/#installing-prerequisites","text":"Have an OS Template with Kubernetes Installed Setup Master and Worker Nodes in VMware Install VMware Cloud Provider Interface (CPI) For snapshot support install Kubernetes CSI Snapshot Controller","title":"Installing Prerequisites"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/","text":"Setting up a base template - Create Ubuntu 18.04 LTS Template On a separate system install PowerShell, PowerCLI , govc and download the base OS to use for your Kubernetes installation. It is recommended to use Ubuntu 18.04LTS as the base OS for Kubernetes on VMware. DHCP should be configured on the subnet that the Kubernetes nodes are being deployed to. Creating the template follows this guide Configure PowerCLI PowerCLI will need to be configured to bypass certificate validation and the system proxy. # Install VMware PowerShell Module PS /> Install-Module -name VMware.PowerCLI -Confirm:$false # Disable VMware Customer Experience Improvement Program participation PS /> Set-PowerCLIConfiguration -ProxyPolicy NoProxy -Scope User -ParticipateInCEIP $false -Confirm:$false Scope ProxyPolicy DefaultVIServerMode InvalidCertificateAction DisplayDeprecationWarnings WebOperationTimeout Seconds ----- ----------- ------------------- ------------------------ -------------------------- ------------------- Session NoProxy Multiple Unset True 300 User AllUsers # Check Version PS /> Get-Module -Name VMware.VimAutomation.Sdk | select Name, Version Name Version ---- ------- VMware.VimAutomation.Sdk 11.5.0.14898111 # Ignore certificate errors PS /> Set-PowerCLIConfiguration -InvalidCertificateAction Ignore -Confirm:$false Scope ProxyPolicy DefaultVIServerMode InvalidCertificateAction DisplayDeprecationWarnings WebOperationTimeout Seconds ----- ----------- ------------------- ------------------------ -------------------------- ------------------- Session NoProxy Multiple Ignore True 300 User Ignore AllUsers Setup govc Configure some environment variables for govc, these values determine where the Ubuntu image and Kubernetes nodes are placed. $ export GOVC_INSECURE=1 # Don't verify SSL certs on vCenter $ export GOVC_URL=10.10.10.10 # vCenter IP/FQDN $ export GOVC_USERNAME=administrator # vCenter username $ export GOVC_PASSWORD=password # vCenter password $ export GOVC_DATASTORE=Datastore # Default datastore to deploy to $ export GOVC_NETWORK=\"VM Network\" # Default network to deploy to $ export GOVC_RESOURCE_POOL='*/Resources' # Default resource pool to deploy to Check that govc has access to your vcenter $ govc about Name: VMware vCenter Server Vendor: VMware, Inc. Version: 6.7.0 Build: 10244857 OS type: linux-x64 API type: VirtualCenter API version: 6.7.1 Product ID: vpx UUID: 1bd33d4e-555f-4d8b-9b77-8d155f612155 Create OVF Spec from Ubuntu OVA $ govc import.spec ~/Downloads/ubuntu-18.04-server-cloudimg-amd64.ova | python -m json.tool > ubuntu.json Create a new SSH key and store it ~/.ssh Setting SSH key for Linux Setting SSH key for Windows Customize the OVF Spec by updating \"Value\" for these key fields hostname, public-keys, password, network and name { \"DiskProvisioning\": \"thin\", \"IPAllocationPolicy\": \"dhcpPolicy\", \"IPProtocol\": \"IPv4\", \"PropertyMapping\": [ { \"Key\": \"instance-id\", \"Value\": \"id-ovf\" }, { \"Key\": \"hostname\", \"Value\": \"Ubuntu1804Template\" }, { \"Key\": \"seedfrom\", \"Value\": \"\" }, { \"Key\": \"public-keys\", \"Value\": \"ssh-rsa [YOUR PUBLIC KEY] [username]\" }, { \"Key\": \"user-data\", \"Value\": \"\" }, { \"Key\": \"password\", \"Value\": \"password\" } ], \"NetworkMapping\": [ { \"Name\": \"VM Network\", \"Network\": \"VM Network\" } ], \"MarkAsTemplate\": false, \"PowerOn\": false, \"InjectOvfEnv\": false, \"WaitForIP\": false, \"Name\": \"Ubuntu1804Template\" } If using Windows you'll need to convert the ubuntu.json to unix format using dos2unix Deploy the OVA Import the OVA $ govc import.ova -options=ubuntu.json ~/Downloads/ubuntu-18.04-server-cloudimg-amd64.ova Set the VM's resources $ govc vm.change -vm Ubuntu1804Template -c 4 -m 4096 -e=\"disk.enableUUID=1\" $ govc vm.disk.change -vm Ubuntu1804Template -disk.label \"Hard disk 1\" -size 60G Power on the VM and wait until IP is assigned $ govc vm.power -on=true Ubuntu1804Template $ govc vm.info Ubuntu1804Template Name: Ubuntu1804Template Path: /vSAN-DC/vm/Discovered virtual machine/Ubuntu1804Template UUID: 42392966-8d21-ceda-5f23-28584c18703b Guest name: Ubuntu Linux (64-bit) Memory: 1024MB CPU: 2 vCPU(s) Power state: poweredOn Boot time: 2019-01-25 18:28:21.978093 +0000 UTC IP address: 10.198.17.85 Host: 10.198.17.31 Update the VM with open-vm-tools package $ ssh ubuntu@10.198.17.85 $ sudo apt update $ sudo apt install open-vm-tools -y $ sudo apt upgrade -y $ sudo apt autoremove -y Remove grub.d from the VMs before you upgrade the hardware version. $ sudo rm -rf /etc/default/grub.d $ sudo update-grub $ sudo shutdown now Set the Hardware version of the VM to 15. This is required for VMware to recognize the VM as a Kubernetes node. $ govc vm.upgrade -version=15 -vm '/datacenterName/vm/Ubuntu1804Template' Verify that the VM's Hardware version was set $ govc vm.option.info '/datacenterName/vm/Ubuntu1804Template' | grep HwVersion HwVersion: 15 Install Docker Power the VM back on and install the libraries needed to install docker $ govc vm.power -on=true Ubuntu1804Template $ govc vm.info Ubuntu1804Template $ ssh ubuntu@10.198.17.85 $ sudo apt install ca-certificates software-properties-common apt-transport-https curl -y Add Docker offical GPG Key $ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add Docker apt repository $ sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" $ sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Create the file /etc/apt/sources.list.d/kubernetes.list to point to the Kubernetes repo # Contents of /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main Update apt with the new repositories just added $ sudo apt update Install DockerCE # install the appropriate version of docker based on the k8s version, refer to the example given below for k8s 1.17 $ sudo apt install docker-ce=5:19.03.4~3-0~ubuntu-bionic -y Setup Docker's Daemon file /etc/docker/daemon.json # Contents of /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } Restart the Docker service using the new settings $ sudo mkdir -p /etc/systemd/system/docker.service.d $ sudo systemctl daemon-reload $ sudo systemctl restart docker $ sudo systemctl status docker docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2019-09-06 12:37:27 UTC; 4min 15s ago $ sudo docker info | egrep \"Server Version|Cgroup Driver\" Server Version: 19.03.4-ce Cgroup Driver: systemd Install Kubernetes # install the desired version of k8s, similar to the example below $ sudo apt install -qy kubeadm=1.17.0-00 kubelet=1.17.0-00 kubectl=1.17.0-00 Hold Kubernetes packages at their installed version so they do not upgrade unexpectedly on an apt upgrade $ sudo apt-mark hold kubelet kubeadm kubectl Install Network for Pod nodes This example uses Flannel for the kubernetes cluster network. Flannel needs to have IPv4 traffic bridged to iptables chains $ sudo sysctl net.bridge.bridge-nf-call-iptables=1 Disable cloud-init on VM and use VMware Guest Customization specs instead $ sudo cloud-init clean --logs $ sudo touch /etc/cloud/cloud-init.disabled $ sudo rm -rf /etc/netplan/50-cloud-init.yaml $ sudo apt purge cloud-init -y $ sudo apt autoremove -y # Don't clear /tmp $ sudo sed -i 's/D \\/tmp 1777 root root -/#D \\/tmp 1777 root root -/g' /usr/lib/tmpfiles.d/tmp.conf # Remove cloud-init and rely on dbus for open-vm-tools $ sudo sed -i 's/Before=cloud-init-local.service/After=dbus.service/g' /lib/systemd/system/open-vm-tools.service # cleanup current ssh keys so templated VMs get fresh key $ sudo rm -f /etc/ssh/ssh_host_* Add check for ssh keys on reboot, create /etc/rc.local # Contents of /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. test -f /etc/ssh/ssh_host_dsa_key || dpkg-reconfigure openssh-server exit 0 Clean the VM before making it a template # make the script executable $ sudo chmod +x /etc/rc.local # cleanup apt $ sudo apt clean # reset the machine-id (DHCP leases in 18.04 are generated based on this... not MAC...) $ echo \"\" | sudo tee /etc/machine-id >/dev/null # disable swap for K8s $ sudo swapoff --all $ sudo sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab # cleanup shell history and shutdown for templating $ history -c $ history -w $ sudo shutdown -h now Turn the VM into a Template $ govc vm.markastemplate Ubuntu1804Template Define a Customization spec with DNS, Domain, and OSType defined $ Connect-VIServer <vCenter IP> -User <vCenter Username> -Password <vCenter Password> $ New-OSCustomizationSpec -Name CustomizationName -OSType Linux -DnsServer 1.1.1.1, 8.8.8.8 -DnsSuffix mydomain.net -Domain mydomain.net -NamingScheme vm Name Description Type OSType LastUpdate Server ---- ----------- ---- ------ ---------- ------ CustomizationName Persistent Linux 27/01/2019 21:43:40 <vCenter IP> Create Kubernetes master and worker nodes $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-master $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-worker1 $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-worker2 $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-worker3 Now that there are some Kubernetes nodes created it is time to setup a master node and the worker nodes.","title":"Setup Base Template"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#setting-up-a-base-template-create-ubuntu-1804-lts-template","text":"On a separate system install PowerShell, PowerCLI , govc and download the base OS to use for your Kubernetes installation. It is recommended to use Ubuntu 18.04LTS as the base OS for Kubernetes on VMware. DHCP should be configured on the subnet that the Kubernetes nodes are being deployed to. Creating the template follows this guide","title":"Setting up a base template - Create Ubuntu 18.04 LTS Template"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#configure-powercli","text":"PowerCLI will need to be configured to bypass certificate validation and the system proxy. # Install VMware PowerShell Module PS /> Install-Module -name VMware.PowerCLI -Confirm:$false # Disable VMware Customer Experience Improvement Program participation PS /> Set-PowerCLIConfiguration -ProxyPolicy NoProxy -Scope User -ParticipateInCEIP $false -Confirm:$false Scope ProxyPolicy DefaultVIServerMode InvalidCertificateAction DisplayDeprecationWarnings WebOperationTimeout Seconds ----- ----------- ------------------- ------------------------ -------------------------- ------------------- Session NoProxy Multiple Unset True 300 User AllUsers # Check Version PS /> Get-Module -Name VMware.VimAutomation.Sdk | select Name, Version Name Version ---- ------- VMware.VimAutomation.Sdk 11.5.0.14898111 # Ignore certificate errors PS /> Set-PowerCLIConfiguration -InvalidCertificateAction Ignore -Confirm:$false Scope ProxyPolicy DefaultVIServerMode InvalidCertificateAction DisplayDeprecationWarnings WebOperationTimeout Seconds ----- ----------- ------------------- ------------------------ -------------------------- ------------------- Session NoProxy Multiple Ignore True 300 User Ignore AllUsers","title":"Configure PowerCLI"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#setup-govc","text":"Configure some environment variables for govc, these values determine where the Ubuntu image and Kubernetes nodes are placed. $ export GOVC_INSECURE=1 # Don't verify SSL certs on vCenter $ export GOVC_URL=10.10.10.10 # vCenter IP/FQDN $ export GOVC_USERNAME=administrator # vCenter username $ export GOVC_PASSWORD=password # vCenter password $ export GOVC_DATASTORE=Datastore # Default datastore to deploy to $ export GOVC_NETWORK=\"VM Network\" # Default network to deploy to $ export GOVC_RESOURCE_POOL='*/Resources' # Default resource pool to deploy to Check that govc has access to your vcenter $ govc about Name: VMware vCenter Server Vendor: VMware, Inc. Version: 6.7.0 Build: 10244857 OS type: linux-x64 API type: VirtualCenter API version: 6.7.1 Product ID: vpx UUID: 1bd33d4e-555f-4d8b-9b77-8d155f612155 Create OVF Spec from Ubuntu OVA $ govc import.spec ~/Downloads/ubuntu-18.04-server-cloudimg-amd64.ova | python -m json.tool > ubuntu.json Create a new SSH key and store it ~/.ssh Setting SSH key for Linux Setting SSH key for Windows Customize the OVF Spec by updating \"Value\" for these key fields hostname, public-keys, password, network and name { \"DiskProvisioning\": \"thin\", \"IPAllocationPolicy\": \"dhcpPolicy\", \"IPProtocol\": \"IPv4\", \"PropertyMapping\": [ { \"Key\": \"instance-id\", \"Value\": \"id-ovf\" }, { \"Key\": \"hostname\", \"Value\": \"Ubuntu1804Template\" }, { \"Key\": \"seedfrom\", \"Value\": \"\" }, { \"Key\": \"public-keys\", \"Value\": \"ssh-rsa [YOUR PUBLIC KEY] [username]\" }, { \"Key\": \"user-data\", \"Value\": \"\" }, { \"Key\": \"password\", \"Value\": \"password\" } ], \"NetworkMapping\": [ { \"Name\": \"VM Network\", \"Network\": \"VM Network\" } ], \"MarkAsTemplate\": false, \"PowerOn\": false, \"InjectOvfEnv\": false, \"WaitForIP\": false, \"Name\": \"Ubuntu1804Template\" } If using Windows you'll need to convert the ubuntu.json to unix format using dos2unix","title":"Setup govc"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#deploy-the-ova","text":"Import the OVA $ govc import.ova -options=ubuntu.json ~/Downloads/ubuntu-18.04-server-cloudimg-amd64.ova Set the VM's resources $ govc vm.change -vm Ubuntu1804Template -c 4 -m 4096 -e=\"disk.enableUUID=1\" $ govc vm.disk.change -vm Ubuntu1804Template -disk.label \"Hard disk 1\" -size 60G Power on the VM and wait until IP is assigned $ govc vm.power -on=true Ubuntu1804Template $ govc vm.info Ubuntu1804Template Name: Ubuntu1804Template Path: /vSAN-DC/vm/Discovered virtual machine/Ubuntu1804Template UUID: 42392966-8d21-ceda-5f23-28584c18703b Guest name: Ubuntu Linux (64-bit) Memory: 1024MB CPU: 2 vCPU(s) Power state: poweredOn Boot time: 2019-01-25 18:28:21.978093 +0000 UTC IP address: 10.198.17.85 Host: 10.198.17.31 Update the VM with open-vm-tools package $ ssh ubuntu@10.198.17.85 $ sudo apt update $ sudo apt install open-vm-tools -y $ sudo apt upgrade -y $ sudo apt autoremove -y Remove grub.d from the VMs before you upgrade the hardware version. $ sudo rm -rf /etc/default/grub.d $ sudo update-grub $ sudo shutdown now Set the Hardware version of the VM to 15. This is required for VMware to recognize the VM as a Kubernetes node. $ govc vm.upgrade -version=15 -vm '/datacenterName/vm/Ubuntu1804Template' Verify that the VM's Hardware version was set $ govc vm.option.info '/datacenterName/vm/Ubuntu1804Template' | grep HwVersion HwVersion: 15","title":"Deploy the OVA"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#install-docker","text":"Power the VM back on and install the libraries needed to install docker $ govc vm.power -on=true Ubuntu1804Template $ govc vm.info Ubuntu1804Template $ ssh ubuntu@10.198.17.85 $ sudo apt install ca-certificates software-properties-common apt-transport-https curl -y Add Docker offical GPG Key $ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add Docker apt repository $ sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" $ sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Create the file /etc/apt/sources.list.d/kubernetes.list to point to the Kubernetes repo # Contents of /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main Update apt with the new repositories just added $ sudo apt update Install DockerCE # install the appropriate version of docker based on the k8s version, refer to the example given below for k8s 1.17 $ sudo apt install docker-ce=5:19.03.4~3-0~ubuntu-bionic -y Setup Docker's Daemon file /etc/docker/daemon.json # Contents of /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } Restart the Docker service using the new settings $ sudo mkdir -p /etc/systemd/system/docker.service.d $ sudo systemctl daemon-reload $ sudo systemctl restart docker $ sudo systemctl status docker docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2019-09-06 12:37:27 UTC; 4min 15s ago $ sudo docker info | egrep \"Server Version|Cgroup Driver\" Server Version: 19.03.4-ce Cgroup Driver: systemd","title":"Install Docker"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#install-kubernetes","text":"# install the desired version of k8s, similar to the example below $ sudo apt install -qy kubeadm=1.17.0-00 kubelet=1.17.0-00 kubectl=1.17.0-00 Hold Kubernetes packages at their installed version so they do not upgrade unexpectedly on an apt upgrade $ sudo apt-mark hold kubelet kubeadm kubectl","title":"Install Kubernetes"},{"location":"driver-deployment/prerequisites-deployment/setup-base-template/#install-network-for-pod-nodes","text":"This example uses Flannel for the kubernetes cluster network. Flannel needs to have IPv4 traffic bridged to iptables chains $ sudo sysctl net.bridge.bridge-nf-call-iptables=1 Disable cloud-init on VM and use VMware Guest Customization specs instead $ sudo cloud-init clean --logs $ sudo touch /etc/cloud/cloud-init.disabled $ sudo rm -rf /etc/netplan/50-cloud-init.yaml $ sudo apt purge cloud-init -y $ sudo apt autoremove -y # Don't clear /tmp $ sudo sed -i 's/D \\/tmp 1777 root root -/#D \\/tmp 1777 root root -/g' /usr/lib/tmpfiles.d/tmp.conf # Remove cloud-init and rely on dbus for open-vm-tools $ sudo sed -i 's/Before=cloud-init-local.service/After=dbus.service/g' /lib/systemd/system/open-vm-tools.service # cleanup current ssh keys so templated VMs get fresh key $ sudo rm -f /etc/ssh/ssh_host_* Add check for ssh keys on reboot, create /etc/rc.local # Contents of /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. test -f /etc/ssh/ssh_host_dsa_key || dpkg-reconfigure openssh-server exit 0 Clean the VM before making it a template # make the script executable $ sudo chmod +x /etc/rc.local # cleanup apt $ sudo apt clean # reset the machine-id (DHCP leases in 18.04 are generated based on this... not MAC...) $ echo \"\" | sudo tee /etc/machine-id >/dev/null # disable swap for K8s $ sudo swapoff --all $ sudo sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab # cleanup shell history and shutdown for templating $ history -c $ history -w $ sudo shutdown -h now Turn the VM into a Template $ govc vm.markastemplate Ubuntu1804Template Define a Customization spec with DNS, Domain, and OSType defined $ Connect-VIServer <vCenter IP> -User <vCenter Username> -Password <vCenter Password> $ New-OSCustomizationSpec -Name CustomizationName -OSType Linux -DnsServer 1.1.1.1, 8.8.8.8 -DnsSuffix mydomain.net -Domain mydomain.net -NamingScheme vm Name Description Type OSType LastUpdate Server ---- ----------- ---- ------ ---------- ------ CustomizationName Persistent Linux 27/01/2019 21:43:40 <vCenter IP> Create Kubernetes master and worker nodes $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-master $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-worker1 $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-worker2 $ govc vm.clone -vm Ubuntu1804Template -customization=CustomizationName k8s-worker3 Now that there are some Kubernetes nodes created it is time to setup a master node and the worker nodes.","title":"Install Network for Pod nodes"},{"location":"driver-deployment/prerequisites-deployment/setup-master-and-worker-nodes/","text":"Setup the master and worker nodes Follow the VMware guide for installing master and worker nodes here After setting up the master nodes and worker nodes, the VMware CPI will be installed in the following section.","title":"Setup Master and Worker Nodes"},{"location":"driver-deployment/prerequisites-deployment/setup-master-and-worker-nodes/#setup-the-master-and-worker-nodes","text":"Follow the VMware guide for installing master and worker nodes here After setting up the master nodes and worker nodes, the VMware CPI will be installed in the following section.","title":"Setup the master and worker nodes"}]}